<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hadoop之MapReduce详解</title>
    <url>/2020/01/09/Hadoop%E4%B9%8BMapReduce%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="第1章-MapReduce概述"><a href="#第1章-MapReduce概述" class="headerlink" title="第1章 MapReduce概述"></a>第1章 MapReduce概述</h2><h3 id="1-1-MapReduce定义"><a href="#1-1-MapReduce定义" class="headerlink" title="1.1 MapReduce定义"></a>1.1 MapReduce定义</h3><p>MapReduce是<strong>一个分布式运算程序的编程框架</strong>，是用户开发“基于Hadoop的数据分析应用”的核心框架。<br>MapReduce核心功能是将<strong>用户编写的业务逻辑代码</strong>和<strong>自带默认组件</strong>整合成一个完整的<strong>分布式运算程序</strong>，并发运行在一个Hadoop集群上。</p>
<h3 id="1-2-MapReduce优缺点"><a href="#1-2-MapReduce优缺点" class="headerlink" title="1.2 MapReduce优缺点"></a>1.2 MapReduce优缺点</h3><h4 id="1-2-1优点"><a href="#1-2-1优点" class="headerlink" title="1.2.1优点"></a>1.2.1优点</h4><h5 id="1-MapReduce易于编程"><a href="#1-MapReduce易于编程" class="headerlink" title="1.MapReduce易于编程"></a>1.MapReduce易于编程</h5><p><strong>它简单的实现一些接口，就可以完成一个分布式程序</strong>，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。</p>
<h5 id="2-良好的扩展性"><a href="#2-良好的扩展性" class="headerlink" title="2.良好的扩展性"></a>2.良好的扩展性</h5><p>当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力</p>
<h5 id="3-高容错性"><a href="#3-高容错性" class="headerlink" title="3.高容错性"></a>3.高容错性</h5><p>MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如<strong>其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败</strong>，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。</p>
<h5 id="4-适合PB级以上海量数据的离线处理"><a href="#4-适合PB级以上海量数据的离线处理" class="headerlink" title="4.适合PB级以上海量数据的离线处理"></a>4.适合PB级以上海量数据的离线处理</h5><p>可以实现上千台服务器集群并发工作，提供数据处理能力。</p>
<h4 id="1-2-2缺点"><a href="#1-2-2缺点" class="headerlink" title="1.2.2缺点"></a>1.2.2缺点</h4><h5 id="1-不擅长实时计算"><a href="#1-不擅长实时计算" class="headerlink" title="1.不擅长实时计算"></a>1.不擅长实时计算</h5><p>MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。</p>
<h5 id="2-不擅长流式计算"><a href="#2-不擅长流式计算" class="headerlink" title="2.不擅长流式计算"></a>2.不擅长流式计算</h5><p>流式计算的输入数据是动态的，而MapReduce的<strong>输入数据集是静态的</strong>，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。</p>
<h5 id="3-不擅长DAG（有向图）计算"><a href="#3-不擅长DAG（有向图）计算" class="headerlink" title="3.不擅长DAG（有向图）计算"></a>3.不擅长DAG（有向图）计算</h5><p>多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，<strong>每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO</strong>，导致性能非常的低下</p>
<h3 id="1-3-MapReduce核心思想"><a href="#1-3-MapReduce核心思想" class="headerlink" title="1.3 MapReduce核心思想"></a>1.3 MapReduce核心思想</h3><p><img src="/2020/01/09/Hadoop%E4%B9%8BMapReduce%E8%AF%A6%E8%A7%A3/E:%5Cblog%5Csource_posts%5CHadoop%E4%B9%8BMapReduce%E8%AF%A6%E8%A7%A3%5C01.png" alt></p>
<ol>
<li>分布式的运算程序往往需要分成至少2个阶段。</li>
<li>第一个阶段的MapTask并发实例，完全并行运行，互不相干。</li>
<li>第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。</li>
<li>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。</li>
</ol>
<a id="more"></a>

<h3 id="1-4-MapReduce进程"><a href="#1-4-MapReduce进程" class="headerlink" title="1.4 MapReduce进程"></a>1.4 MapReduce进程</h3><p>一个完整的MapReduce程序在分布式运行时有三类实例进程</p>
<ul>
<li><strong>MrAppMaster</strong>：负责整个程序的过程调度及状态协调。</li>
<li><strong>MapTask</strong>：负责Map阶段的整个数据处理流程。</li>
<li><strong>ReduceTask</strong>：负责Reduce阶段的整个数据处理流程。</li>
</ul>
<h3 id="1-5常用数据序列化类型"><a href="#1-5常用数据序列化类型" class="headerlink" title="1.5常用数据序列化类型"></a>1.5常用数据序列化类型</h3><table>
<thead>
<tr>
<th><strong>Java类型</strong></th>
<th><strong>Hadoop Writable类型</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>Byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>Int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>Float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>Long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>Double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>Map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>Array</td>
<td>ArrayWritable</td>
</tr>
</tbody></table>
<h3 id="1-6-MapReduce编程规范"><a href="#1-6-MapReduce编程规范" class="headerlink" title="1.6 MapReduce编程规范"></a>1.6 MapReduce编程规范</h3><p>用户编写的程序分成三个部分：Mapper、Reducer和Driver。</p>
<h4 id="1-6-1-Mapper阶段"><a href="#1-6-1-Mapper阶段" class="headerlink" title="1.6.1 Mapper阶段"></a>1.6.1 Mapper阶段</h4><p> （1）用户自定义的Mapper要继承自己的父类</p>
<p> （2）Mapper的输入数据是KV对的形式（KV的类型可自定义）</p>
<p> （3）Mapper中的业务逻辑写在map()方法中</p>
<p> （4）Mapper的输出数据是KV对的形式（KV的类型可自定义）</p>
<p> （5）map()方法（MapTask进程）对每一个&lt;K,V&gt;调用一次</p>
<h4 id="1-6-2-Reducer阶段"><a href="#1-6-2-Reducer阶段" class="headerlink" title="1.6.2 Reducer阶段"></a>1.6.2 Reducer阶段</h4><p>（1）用户自定义的Reducer要继承自己的父类</p>
<p>（2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV</p>
<p>（3）Reducer的业务逻辑写在reduce()方法中</p>
<p>（4）ReduceTask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法</p>
<h4 id="1-6-3-Driver阶段"><a href="#1-6-3-Driver阶段" class="headerlink" title="1.6.3 Driver阶段"></a>1.6.3 Driver阶段</h4><p>相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象</p>
<h3 id="1-7-WordCount案例实操"><a href="#1-7-WordCount案例实操" class="headerlink" title="1.7 WordCount案例实操"></a>1.7 WordCount案例实操</h3><h4 id="1-7-1案例需求"><a href="#1-7-1案例需求" class="headerlink" title="1.7.1案例需求"></a>1.7.1案例需求</h4><p>统计下面文档中每个单词的出现的总次数</p>
<p><img src="/2020/01/09/Hadoop%E4%B9%8BMapReduce%E8%AF%A6%E8%A7%A3/02.png" alt></p>
<h4 id="1-7-2需求分析"><a href="#1-7-2需求分析" class="headerlink" title="1.7.2需求分析"></a>1.7.2需求分析</h4><p>按照MapReduce编程规范，分别编写Mapper，Reducer，Driver</p>
<p><img src="/2020/01/09/Hadoop%E4%B9%8BMapReduce%E8%AF%A6%E8%A7%A3/03.png" alt></p>
<h4 id="1-7-3-环境准备"><a href="#1-7-3-环境准备" class="headerlink" title="1.7.3 环境准备"></a>1.7.3 环境准备</h4><p><strong>（1）创建maven工程</strong></p>
<p><strong>（2）在pom.xml文件中添加如下依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>（3）在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<h4 id="1-7-4-编写程序"><a href="#1-7-4-编写程序" class="headerlink" title="1.7.4 编写程序"></a>1.7.4 编写程序</h4><p><strong>（1）编写Mapper类</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hp.wordcount;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WcReducer</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span><span class="comment">//(此处重写map函数可以按 ctrl+o 然后选择map)</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//拿到这一行数据</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//按照空格切分数据</span></span><br><span class="line">        String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历数组，把单词变成（word,1)的形式交给框架环境</span></span><br><span class="line">        <span class="keyword">for</span>(String word :words)&#123;</span><br><span class="line">            <span class="keyword">this</span>.word.set(word);</span><br><span class="line">            context.write(<span class="keyword">this</span>.word,<span class="keyword">this</span>.one);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>个人笔记：Mapper&lt;LongWritable, Text, Text, IntWritable&gt;中LongWritable表示取一行数据的偏移量</p>
<p>Text是单词文本，IntWritable是用于计数的one,contex表示当前框架环境，用于将map结果返回。</p>
</blockquote>
<p><strong>（2）编写Reducer类</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hp.wordcount;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WcMapper</span>  <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>,<span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable total = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.reduce(key, values, context);</span><br><span class="line">       <span class="comment">//做累加</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//包装结果并输出</span></span><br><span class="line">        total.set(sum);</span><br><span class="line">        context.write(key,total);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>个人学习笔记</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop之HDFS详解</title>
    <url>/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="第1章-HDFS概述"><a href="#第1章-HDFS概述" class="headerlink" title="第1章 HDFS概述"></a>第1章 HDFS概述</h2><h3 id="1-1-HDFS产生背景"><a href="#1-1-HDFS产生背景" class="headerlink" title="1.1 HDFS产生背景"></a>1.1 HDFS产生背景</h3><p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。</p>
<h3 id="1-2HDFS的定义"><a href="#1-2HDFS的定义" class="headerlink" title="1.2HDFS的定义"></a>1.2HDFS的定义</h3><p>HDFS（Hadoop Distributed File System），它是一个<strong>文件系统</strong>，用于存储文件，通过目录树来定位文件；<strong>其次，它是分布式的</strong>，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<br><strong>HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。</strong>适合用来做数据分析，并不适合用来做网盘应用。</p>
<h3 id="1-3HDFS优缺点"><a href="#1-3HDFS优缺点" class="headerlink" title="1.3HDFS优缺点"></a>1.3HDFS优缺点</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/01.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/E:%5Cblog%5Csource_posts%5CHadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3%5C02.png" alt></p>
<h3 id="1-4-HDFS组成架构"><a href="#1-4-HDFS组成架构" class="headerlink" title="1.4 HDFS组成架构"></a>1.4 HDFS组成架构</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/03.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/04.png" alt></p>
<a id="more"></a>

<h3 id="1-5-HDFS文件块大小"><a href="#1-5-HDFS文件块大小" class="headerlink" title="1.5 HDFS文件块大小"></a>1.5 HDFS文件块大小</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/05.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/06.png" alt></p>
<h2 id="第2章-HDFS的Shell操作"><a href="#第2章-HDFS的Shell操作" class="headerlink" title="第2章 HDFS的Shell操作"></a>第2章 HDFS的Shell操作</h2><h3 id="2-1基本语法"><a href="#2-1基本语法" class="headerlink" title="2.1基本语法"></a>2.1基本语法</h3><p><strong>bin/hadoop fs</strong> 具体命令  或者 <strong>bin/hdfs dfs</strong> 具体命令</p>
<h3 id="2-2常用命令实操"><a href="#2-2常用命令实操" class="headerlink" title="2.2常用命令实操"></a>2.2常用命令实操</h3><h4 id="（0）启动Hadoop集群"><a href="#（0）启动Hadoop集群" class="headerlink" title="（0）启动Hadoop集群"></a>（0）启动Hadoop集群</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[hep@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="（1）-help：输出这个命令参数"><a href="#（1）-help：输出这个命令参数" class="headerlink" title="（1）-help：输出这个命令参数"></a>（1）-help：输出这个命令参数</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm</span><br></pre></td></tr></table></figure>

<h4 id="（2）-ls-显示目录信息"><a href="#（2）-ls-显示目录信息" class="headerlink" title="（2）-ls: 显示目录信息"></a>（2）-ls: 显示目录信息</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /</span><br></pre></td></tr></table></figure>

<h4 id="（3）-mkdir：在HDFS上创建目录"><a href="#（3）-mkdir：在HDFS上创建目录" class="headerlink" title="（3）-mkdir：在HDFS上创建目录"></a>（3）-mkdir：在HDFS上创建目录</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<h4 id="（4）-moveFromLocal：从本地剪切粘贴到HDFS"><a href="#（4）-moveFromLocal：从本地剪切粘贴到HDFS" class="headerlink" title="（4）-moveFromLocal：从本地剪切粘贴到HDFS"></a>（4）-moveFromLocal：从本地剪切粘贴到HDFS</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch kongming.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<h4 id="（5）-appendToFile：追加一个文件到已经存在的文件末尾"><a href="#（5）-appendToFile：追加一个文件到已经存在的文件末尾" class="headerlink" title="（5）-appendToFile：追加一个文件到已经存在的文件末尾"></a>（5）-appendToFile：追加一个文件到已经存在的文件末尾</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch liubei.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ vi liubei.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（6）-cat：显示文件内容"><a href="#（6）-cat：显示文件内容" class="headerlink" title="（6）-cat：显示文件内容"></a>（6）-cat：显示文件内容</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（7）-chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"><a href="#（7）-chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限" class="headerlink" title="（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"></a>（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -chown hep:hep  /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"><a href="#（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去" class="headerlink" title="（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"></a>（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /</span><br></pre></td></tr></table></figure>

<h4 id="（9）-copyToLocal：从HDFS拷贝到本地"><a href="#（9）-copyToLocal：从HDFS拷贝到本地" class="headerlink" title="（9）-copyToLocal：从HDFS拷贝到本地"></a>（9）-copyToLocal：从HDFS拷贝到本地</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>

<h4 id="（10）-cp-：从HDFS的一个路径拷贝到HDFS的另一个路径"><a href="#（10）-cp-：从HDFS的一个路径拷贝到HDFS的另一个路径" class="headerlink" title="（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径"></a>（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt</span><br></pre></td></tr></table></figure>

<h4 id="（11）-mv：在HDFS目录中移动文件"><a href="#（11）-mv：在HDFS目录中移动文件" class="headerlink" title="（11）-mv：在HDFS目录中移动文件"></a>（11）-mv：在HDFS目录中移动文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/</span><br></pre></td></tr></table></figure>

<h4 id="（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地"><a href="#（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地" class="headerlink" title="（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地"></a>（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>

<h4 id="（13）-getmerge：合并下载多个文件"><a href="#（13）-getmerge：合并下载多个文件" class="headerlink" title="（13）-getmerge：合并下载多个文件"></a>（13）-getmerge：合并下载多个文件</h4><p>比如HDFS的目录 /user/hep/test下有多个文件:log.1, log.2,log.3,…</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/hep/test/* ./zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<h4 id="（14）-put：等同于copyFromLocal"><a href="#（14）-put：等同于copyFromLocal" class="headerlink" title="（14）-put：等同于copyFromLocal"></a>（14）-put：等同于copyFromLocal</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/hep/test/</span><br></pre></td></tr></table></figure>

<h4 id="（15）-tail：显示一个文件的末尾"><a href="#（15）-tail：显示一个文件的末尾" class="headerlink" title="（15）-tail：显示一个文件的末尾"></a>（15）-tail：显示一个文件的末尾</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（16）-rm：删除文件或文件夹"><a href="#（16）-rm：删除文件或文件夹" class="headerlink" title="（16）-rm：删除文件或文件夹"></a>（16）-rm：删除文件或文件夹</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/hep/test/jinlian2.txt</span><br></pre></td></tr></table></figure>

<h4 id="（17）-rmdir：删除空目录"><a href="#（17）-rmdir：删除空目录" class="headerlink" title="（17）-rmdir：删除空目录"></a>（17）-rmdir：删除空目录</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test</span><br></pre></td></tr></table></figure>

<h4 id="（18）-du统计文件夹的大小信息"><a href="#（18）-du统计文件夹的大小信息" class="headerlink" title="（18）-du统计文件夹的大小信息"></a>（18）-du统计文件夹的大小信息</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/hep/test</span><br></pre></td></tr></table></figure>

<p><strong>结果：</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.7 K /user/hep/test</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/hep/test</span><br></pre></td></tr></table></figure>

<p><strong>结果：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.3 K &#x2F;user&#x2F;hep&#x2F;test&#x2F;README.txt</span><br><span class="line">15   &#x2F;user&#x2F;hep&#x2F;test&#x2F;jinlian.txt</span><br><span class="line">1.4 K &#x2F;user&#x2F;hep&#x2F;test&#x2F;zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<h4 id="（19）-setrep：设置HDFS中文件的副本数量"><a href="#（19）-setrep：设置HDFS中文件的副本数量" class="headerlink" title="（19）-setrep：设置HDFS中文件的副本数量"></a>（19）-setrep：设置HDFS中文件的副本数量</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 &#x2F;sanguo&#x2F;shuguo&#x2F;kongming.txt</span><br></pre></td></tr></table></figure>

<p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
<h2 id="第3章-HDFS客户端操作"><a href="#第3章-HDFS客户端操作" class="headerlink" title="第3章 HDFS客户端操作"></a>第3章 HDFS客户端操作</h2><h3 id="3-1-HDFS客户端环境准备"><a href="#3-1-HDFS客户端环境准备" class="headerlink" title="3.1 HDFS客户端环境准备"></a>3.1 HDFS客户端环境准备</h3><h4 id="1．拷贝对应的编译后的hadoop-jar包到非中文路径"><a href="#1．拷贝对应的编译后的hadoop-jar包到非中文路径" class="headerlink" title="1．拷贝对应的编译后的hadoop jar包到非中文路径"></a>1．拷贝对应的编译后的hadoop jar包到非中文路径</h4><h4 id="2．配置HADOOP-HOME环境变量"><a href="#2．配置HADOOP-HOME环境变量" class="headerlink" title="2．配置HADOOP_HOME环境变量"></a>2．配置HADOOP_HOME环境变量</h4><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/07.png" alt></p>
<h4 id="3-配置Path环境变量"><a href="#3-配置Path环境变量" class="headerlink" title="3.配置Path环境变量"></a>3.配置Path环境变量</h4><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/08.png" alt></p>
<h4 id="4-执行winutils判断是否配置成功"><a href="#4-执行winutils判断是否配置成功" class="headerlink" title="4.执行winutils判断是否配置成功"></a>4.执行winutils判断是否配置成功</h4><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/09.png" alt></p>
<h4 id="5-创建一个Maven工程HdfsClient"><a href="#5-创建一个Maven工程HdfsClient" class="headerlink" title="5.创建一个Maven工程HdfsClient"></a>5.创建一个Maven工程HdfsClient</h4><h4 id="6-导入相应的依赖坐标-日志添加"><a href="#6-导入相应的依赖坐标-日志添加" class="headerlink" title="6.导入相应的依赖坐标+日志添加"></a>6.导入相应的依赖坐标+日志添加</h4><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：如果Eclipse/Idea打印不出日志，在控制台上只显示</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">1.log4j</span>:<span class="string">WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). </span></span><br><span class="line"><span class="meta">2.log4j</span>:<span class="string">WARN Please initialize the log4j system properly. </span></span><br><span class="line"><span class="meta">3.log4j</span>:<span class="string">WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span></span><br></pre></td></tr></table></figure>

<p><strong>需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<h4 id="7-创建包名：com-hep-hdfs"><a href="#7-创建包名：com-hep-hdfs" class="headerlink" title="7.创建包名：com.hep.hdfs"></a>7.创建包名：com.hep.hdfs</h4><h4 id="8-创建HdfsClient类"><a href="#8-创建HdfsClient类" class="headerlink" title="8.创建HdfsClient类"></a>8.创建HdfsClient类</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span>&#123;  </span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line">   <span class="comment">// 1 获取文件系统</span></span><br><span class="line">   Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">   <span class="comment">// 配置在集群上运行</span></span><br><span class="line">   <span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span></span><br><span class="line">   <span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line">   FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line">   <span class="comment">// 2 创建目录</span></span><br><span class="line">   fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/new"</span>));</span><br><span class="line">   <span class="comment">// 3 关闭资源</span></span><br><span class="line">   fs.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2HDFS的API操作"><a href="#3-2HDFS的API操作" class="headerlink" title="3.2HDFS的API操作"></a>3.2HDFS的API操作</h3><h4 id="1-HDFS文件下载"><a href="#1-HDFS文件下载" class="headerlink" title="1.HDFS文件下载"></a>1.HDFS文件下载</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//获取一个HDFS的抽象封装对象</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fileSystem = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line"><span class="comment">//用这个对象操作文件系统</span></span><br><span class="line"> fileSystem.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/test"</span>), <span class="keyword">new</span> Path(<span class="string">"e:\\"</span>));</span><br><span class="line"><span class="comment">//关闭文件系统</span></span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-HDFS文件名更改"><a href="#2-HDFS文件名更改" class="headerlink" title="2.HDFS文件名更改"></a>2.HDFS文件名更改</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">FileSystem fileSystem = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"hep"</span>);</span><br><span class="line">fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/test"</span>), <span class="keyword">new</span> Path(<span class="string">"/test2"</span>));</span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-HDFS文件删除"><a href="#3-HDFS文件删除" class="headerlink" title="3.HDFS文件删除"></a>3.HDFS文件删除</h4><p>此处因为每一次对文件系统进行操作前都需要获取一个HDFS的抽象封装对象，以及在使用完之后关闭文件系统。于是我在测试前写了个@Before和@After，便于后续的测试。之后的API测试均由生成的fs替代。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> FileSystem fs;</span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">before</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"hep"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Before!!!!!"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">after</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    s.close();</span><br><span class="line">    System.out.println(<span class="string">"After!!!!!!!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> delete = fs.delete(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">if</span> (delete) &#123;</span><br><span class="line">        System.out.println(<span class="string">"删除成功"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">"删除失败"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-HDFS文件的追加"><a href="#4-HDFS文件的追加" class="headerlink" title="4.HDFS文件的追加"></a>4.HDFS文件的追加</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    FSDataOutputStream append = fs.append(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>),<span class="number">1024</span>);</span><br><span class="line">    FileInputStream open = <span class="keyword">new</span> FileInputStream(<span class="string">"E://test/1.txt"</span>);</span><br><span class="line">    IOUtils.copyBytes(open,append,<span class="number">1024</span>,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-HDFS文件详情查看以及文件和文件夹判断"><a href="#5-HDFS文件详情查看以及文件和文件夹判断" class="headerlink" title="5.HDFS文件详情查看以及文件和文件夹判断"></a>5.HDFS文件详情查看以及文件和文件夹判断</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    <span class="keyword">for</span>(FileStatus fileStatus : fileStatuses)&#123;</span><br><span class="line">        <span class="keyword">if</span>(fileStatus.isFile())&#123;</span><br><span class="line">            System.out.println(<span class="string">"以下信息是一个文件的信息："</span>);</span><br><span class="line">            System.out.println(fileStatus.getPath());</span><br><span class="line">            System.out.println(fileStatus.getLen());</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"这是一个文件夹"</span>);</span><br><span class="line">            System.out.println(fileStatus.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="6-列出路径下所有的文件块信息"><a href="#6-列出路径下所有的文件块信息" class="headerlink" title="6.列出路径下所有的文件块信息"></a>6.列出路径下所有的文件块信息</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>),<span class="keyword">true</span>);<span class="comment">//此处的true是指递归深入文件夹中的文件夹(只能查文件，因为文件夹没有block)</span></span><br><span class="line">    <span class="keyword">while</span>(files.hasNext()) &#123;</span><br><span class="line">        LocatedFileStatus file = files.next();</span><br><span class="line">        System.out.println(<span class="string">"============================"</span>);</span><br><span class="line">        System.out.println(file.getPath());</span><br><span class="line">        System.out.println(<span class="string">"块信息："</span>);</span><br><span class="line">        BlockLocation[] blockLocations = file.getBlockLocations();</span><br><span class="line">        <span class="keyword">for</span>(BlockLocation blockLocation : blockLocations)&#123;</span><br><span class="line">            String[] hosts = blockLocation.getHosts();</span><br><span class="line">            System.out.print(<span class="string">"块在"</span>);</span><br><span class="line">            <span class="keyword">for</span> (String host : hosts)&#123;</span><br><span class="line">                System.out.println(host+<span class="string">" "</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="7-HDFS文件上传"><a href="#7-HDFS文件上传" class="headerlink" title="7.HDFS文件上传"></a>7.HDFS文件上传</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">()</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    configuration.setInt(<span class="string">"dfs.replication"</span>,<span class="number">1</span>);<span class="comment">//设置配置文件</span></span><br><span class="line">    fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line">    fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"e:/test/1.txt"</span>),<span class="keyword">new</span> Path(<span class="string">"/2.txt"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此处的configuration有自定的配置，若api里有配置文件的设置，api的优先级更高，若没有则看配置文件中的配置。</p>
<h2 id="第4章-HDFS的数据流"><a href="#第4章-HDFS的数据流" class="headerlink" title="第4章 HDFS的数据流"></a>第4章 HDFS的数据流</h2><h3 id="4-1-HDFS写数据流程"><a href="#4-1-HDFS写数据流程" class="headerlink" title="4.1 HDFS写数据流程"></a>4.1 HDFS写数据流程</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/10.png" alt></p>
<ol>
<li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</li>
<li>NameNode返回是否可以上传。</li>
<li>客户端请求第一个 Block上传到哪几个DataNode服务器上。</li>
<li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</li>
<li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</li>
<li>dn1、dn2、dn3逐级应答客户端。</li>
<li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
</ol>
<h3 id="4-2-HDFS读数据流程"><a href="#4-2-HDFS读数据流程" class="headerlink" title="4.2 HDFS读数据流程"></a>4.2 HDFS读数据流程</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/11.png" alt></p>
<ol>
<li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<h3 id="4-3网络拓扑"><a href="#4-3网络拓扑" class="headerlink" title="4.3网络拓扑"></a>4.3网络拓扑</h3><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。</p>
<p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/12.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/13.png" alt></p>
<h3 id="4-4机架感知（副本存储节点选择）"><a href="#4-4机架感知（副本存储节点选择）" class="headerlink" title="4.4机架感知（副本存储节点选择）"></a>4.4机架感知（副本存储节点选择）</h3><blockquote>
<p>For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack.</p>
</blockquote>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/14.png" alt></p>
<h2 id="第5章-NameNode和SecondaryNameNode"><a href="#第5章-NameNode和SecondaryNameNode" class="headerlink" title="第5章 NameNode和SecondaryNameNode"></a>第5章 NameNode和SecondaryNameNode</h2><h3 id="5-1-NN和2NN工作机制"><a href="#5-1-NN和2NN工作机制" class="headerlink" title="5.1 NN和2NN工作机制"></a>5.1 NN和2NN工作机制</h3><blockquote>
<p><strong>思考：NameNode中的元数据是存储在哪里的？</strong></p>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。<strong>因此产生在磁盘中备份元数据的FsImage。</strong></p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失<strong>。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。</strong>这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。<strong>因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</strong></p>
</blockquote>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/15.png" alt></p>
<h4 id="1-第一阶段：NameNode启动"><a href="#1-第一阶段：NameNode启动" class="headerlink" title="1.第一阶段：NameNode启动"></a>1.第一阶段：NameNode启动</h4><p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>（2）客户端对元数据进行增删改的请求。<br>（3）NameNode记录操作日志，更新滚动日志。<br>（4）NameNode在内存中对元数据进行增删改。</p>
<h4 id="2-第二阶段：Secondary-NameNode工作"><a href="#2-第二阶段：Secondary-NameNode工作" class="headerlink" title="2.第二阶段：Secondary NameNode工作"></a>2.第二阶段：Secondary NameNode工作</h4><p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。<br>（2）Secondary NameNode请求执行CheckPoint。<br>（3）NameNode滚动正在写的Edits日志。<br>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br>（6）生成新的镜像文件fsimage.chkpoint。<br>（7）拷贝fsimage.chkpoint到NameNode。<br>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<hr>
<blockquote>
<p><strong>NN和2NN工作机制详解：</strong></p>
<p><strong>Fsimage：</strong>NameNode内存中元数据序列化后形成的文件。</p>
<p><strong>Edits：</strong>记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</p>
<p>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（<strong>所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage</strong>）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</p>
<p>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（<strong>触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了</strong>）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p>
</blockquote>
<h3 id="5-2-Fsimage和Edits解析"><a href="#5-2-Fsimage和Edits解析" class="headerlink" title="5.2 Fsimage和Edits解析"></a>5.2 Fsimage和Edits解析</h3><h4 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h4><p>​    NameNode被格式化之后，将在<strong>/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录</strong>中产生如下文件：<br><strong>（1）Fsimage文件：</strong>HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。<br><strong>（2）Edits文件：</strong>存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。<br><strong>（3）seen_txid文件：</strong>保存的是一个数字，就是最后一个edits_的数字<br>（4）每次NameNode<strong>启动的时候</strong>都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。</p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/16.png" alt></p>
<h4 id="2-oiv查看Fsimage文件"><a href="#2-oiv查看Fsimage文件" class="headerlink" title="2.oiv查看Fsimage文件"></a>2.oiv查看Fsimage文件</h4><h5 id="（1）查看oiv和oev命令"><a href="#（1）查看oiv和oev命令" class="headerlink" title="（1）查看oiv和oev命令"></a>（1）查看oiv和oev命令</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 current]$ hdfs</span><br><span class="line"></span><br><span class="line">**oiv**  apply the offline fsimage viewer to an fsimage</span><br><span class="line"></span><br><span class="line">**oev**  apply the offline edits viewer to an edits file</span><br></pre></td></tr></table></figure>

<h5 id="（2）基本语法"><a href="#（2）基本语法" class="headerlink" title="（2）基本语法"></a>（2）基本语法</h5><p><strong>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</strong></p>
<h5 id="（3）案例实操"><a href="#（3）案例实操" class="headerlink" title="（3）案例实操"></a>（3）案例实操</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 current]$  hdfs oiv -p XML -i fsimage_0000000000000000991 -o /opt/module/hadoop-2.7.2/fsimage.xml</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 current]$  hdfs oev -p XML -i edits_0000000000000000888-0000000000000000940 -o /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure>

<p>将显示的xml文件内容打开,部分显示结果如下。</p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/17.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/18.png" alt></p>
<h3 id="5-3-CheckPoint时间设置"><a href="#5-3-CheckPoint时间设置" class="headerlink" title="5.3 CheckPoint时间设置"></a>5.3 CheckPoint时间设置</h3><h4 id="1-通常情况下，SecondaryNameNode每隔一小时执行一次。"><a href="#1-通常情况下，SecondaryNameNode每隔一小时执行一次。" class="headerlink" title="1.通常情况下，SecondaryNameNode每隔一小时执行一次。"></a>1.通常情况下，SecondaryNameNode每隔一小时执行一次。</h4><p>​    <strong>[hdfs-default.xml]</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="2-自定义设置"><a href="#2-自定义设置" class="headerlink" title="2.自定义设置"></a>2.自定义设置</h4><p>一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="5-4-NameNode故障处理"><a href="#5-4-NameNode故障处理" class="headerlink" title="5.4 NameNode故障处理"></a>5.4 NameNode故障处理</h3><p>NameNode故障后，可以采用如下两种方法恢复数据。</p>
<p><strong>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</strong></p>
<ol>
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 dfs]$ scp -r hep@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>重新启动NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p><strong>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</strong></p>
<ol>
<li>修改hdfs-site.xml中的</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 dfs]$ scp -r hep@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 namesecondary]$ rm -rf in_use.lock</span><br><span class="line"> </span><br><span class="line">[hep@hadoop102 dfs]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 dfs]$ ls</span><br><span class="line">data name namesecondary</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>导入检查点数据（等待一会ctrl+c结束掉）</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>启动NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<h3 id="5-5-集群安全模式"><a href="#5-5-集群安全模式" class="headerlink" title="5.5 集群安全模式"></a>5.5 集群安全模式</h3><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h4><p><strong>NameNode启动:</strong>NameNode启动时，首先将镜像文件（Fsimage）载入内存，并执行编辑日志（Edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的。</p>
<p><strong>DateNode启动:</strong>系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。</p>
<p><strong>安全模式退出判断：</strong>如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。</p>
<h4 id="2-基本语法"><a href="#2-基本语法" class="headerlink" title="2.基本语法"></a>2.基本语法</h4><p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p>
<p>（1）bin/hdfs dfsadmin -safemode get    （功能描述：查看安全模式状态）</p>
<p>（2）bin/hdfs dfsadmin -safemode enter  （功能描述：进入安全模式状态）</p>
<p>（3）bin/hdfs dfsadmin -safemode leave   （功能描述：离开安全模式状态）</p>
<p>（4）bin/hdfs dfsadmin -safemode wait   （功能描述：等待安全模式状态）</p>
<h4 id="3-案例-模拟等待安全模式"><a href="#3-案例-模拟等待安全模式" class="headerlink" title="3.案例-模拟等待安全模式"></a>3.案例-模拟等待安全模式</h4><p>（1）查看当前模式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -safemode get</span><br></pre></td></tr></table></figure>
<p>  Safe mode is OFF<br>（2）先进入安全模式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter</span><br></pre></td></tr></table></figure>
<p> （3）创建并执行下面的脚本<br>在/opt/module/hadoop-2.7.2路径上，编辑一个脚本safemode.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch safemode.sh</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ vim safemode.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode wait</span><br><span class="line">hdfs dfs -put /opt/module/hadoop-2.7.2/README.txt /</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ chmod 777 safemode.sh</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ ./safemode.sh</span><br></pre></td></tr></table></figure>
<p>（4）再打开一个窗口，执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure>
<p>（5）再观察上一个窗口<br> Safe mode is OFF    HDFS集群上已经有上传的数据了。</p>
<h2 id="第6章-DataNode"><a href="#第6章-DataNode" class="headerlink" title="第6章 DataNode"></a>第6章 DataNode</h2><h3 id="6-1-DataNode工作机制"><a href="#6-1-DataNode工作机制" class="headerlink" title="6.1 DataNode工作机制"></a>6.1 DataNode工作机制</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/19.png" alt></p>
<ol>
<li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</li>
<li>DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</li>
<li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li>
<li>集群运行中可以安全加入和退出一些机器。</li>
</ol>
<h3 id="6-2-掉线时限参数设置"><a href="#6-2-掉线时限参数设置" class="headerlink" title="6.2 掉线时限参数设置"></a>6.2 掉线时限参数设置</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/20.png" alt></p>
<p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为<strong>毫秒</strong>，dfs.heartbeat.interval的单位为<strong>秒</strong>。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="6-3-数据完整性"><a href="#6-3-数据完整性" class="headerlink" title="6.3 数据完整性"></a>6.3 数据完整性</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/21.png" alt></p>
<p><strong>DataNode节点保证数据完整性的方法:</strong></p>
<ol>
<li>当DataNode读取Block的时候，它会计算CheckSum。</li>
<li>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</li>
<li>Client读取其他DataNode上的Block。</li>
<li>DataNode在其文件创建后周期验证CheckSum</li>
</ol>
<h3 id="6-4服役新数据节点"><a href="#6-4服役新数据节点" class="headerlink" title="6.4服役新数据节点"></a>6.4服役新数据节点</h3><p>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。</p>
<h4 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1.环境准备"></a>1.环境准备</h4><p><strong>（1）在hadoop104主机上再克隆一台hadoop105主机</strong></p>
<p><strong>（2）修改IP地址和主机名称</strong></p>
<p><strong>（3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）</strong></p>
<p><strong>（4）source一下配置文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop105 hadoop-2.7.2]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="2-服役新节点具体步骤"><a href="#2-服役新节点具体步骤" class="headerlink" title="2.服役新节点具体步骤"></a>2.服役新节点具体步骤</h4><p><strong>（1）直接启动DataNode，即可关联到集群</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</span><br><span class="line"></span><br><span class="line">[hep@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanage</span><br></pre></td></tr></table></figure>

<p><strong>（2）在hadoop105上上传文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop105 hadoop-2.7.2]$ hadoop fs -put /opt/module/hadoop-2.7.2/LICENSE.txt /</span><br></pre></td></tr></table></figure>

<p><strong>（3）如果数据不均衡，可以用命令实现集群的再平衡</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 sbin]$ ./start-balancer.sh</span><br></pre></td></tr></table></figure>

<h3 id="6-5-退役旧数据节点"><a href="#6-5-退役旧数据节点" class="headerlink" title="6.5 退役旧数据节点"></a>6.5 退役旧数据节点</h3><h4 id="1-添加白名单"><a href="#1-添加白名单" class="headerlink" title="1. 添加白名单"></a>1. 添加白名单</h4><p>​    添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。</p>
<p>配置白名单的具体步骤如下：</p>
<p><strong>（1）在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[hep@hadoop102 hadoop]$ touch dfs.hosts</span><br><span class="line">[hep@hadoop102 hadoop]$ vi dfs.hosts</span><br><span class="line"></span><br><span class="line">添加如下主机名称（不添加hadoop105）</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>

<p><strong>（2）在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>（3）配置文件分发</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop]$ xsync hdfs-site.xml</span><br></pre></td></tr></table></figure>

<p><strong>（4）刷新NameNode</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</span><br><span class="line"></span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure>

<p><strong>（5）更新ResourceManager节点</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<p><strong>（6）在web浏览器上查看</strong></p>
<h4 id="2-黑名单退役"><a href="#2-黑名单退役" class="headerlink" title="2. 黑名单退役"></a>2. 黑名单退役</h4><p>在黑名单上面的主机都会被强制退出。</p>
<p><strong>(1)在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop]$ touch dfs.hosts.exclude</span><br><span class="line">[hep@hadoop102 hadoop]$ vi dfs.hosts.exclude</span><br></pre></td></tr></table></figure>

<p>添加如下主机名称（要退役的节点）</p>
<p>hadoop105</p>
<p><strong>(2)在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>(3)刷新NameNode、刷新ResourceManager</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</span><br><span class="line"></span><br><span class="line">Refresh nodes successful</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<p><strong>(4)检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点。</strong></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/22.png" alt></p>
<p><strong>(5)等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役。</strong></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/23.png" alt></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode</span><br><span class="line">stopping datanode</span><br><span class="line"></span><br><span class="line">[hep@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager</span><br><span class="line">stopping nodemanager</span><br></pre></td></tr></table></figure>

<p><strong>(6)如果数据不均衡，可以用命令实现集群的再平衡</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh</span><br></pre></td></tr></table></figure>

<p>​    注意：不允许白名单和黑名单中同时出现同一个主机名称。</p>
<h3 id="6-6-Datanode多目录配置"><a href="#6-6-Datanode多目录配置" class="headerlink" title="6.6 Datanode多目录配置"></a>6.6 Datanode多目录配置</h3><p><strong>1.DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</strong></p>
<p><strong>2.具体配置在 hdfs-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>				 	     	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="第7章-HDFS-2-X新特性"><a href="#第7章-HDFS-2-X新特性" class="headerlink" title="第7章 HDFS 2.X新特性"></a>第7章 HDFS 2.X新特性</h2><h3 id="7-1-集群间数据拷贝"><a href="#7-1-集群间数据拷贝" class="headerlink" title="7.1 集群间数据拷贝"></a>7.1 集群间数据拷贝</h3><h4 id="1．scp实现两个远程主机之间的文件复制"><a href="#1．scp实现两个远程主机之间的文件复制" class="headerlink" title="1．scp实现两个远程主机之间的文件复制"></a>1．scp实现两个远程主机之间的文件复制</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">	 scp -r hello.txt [root@hadoop103:/user/hep/hello.txt](mailto:root@hadoop103:/user/hep/hello.txt)       // 推 push</span><br><span class="line"></span><br><span class="line">​    scp -r [root@hadoop103:/user/hep/hello.txt hello.txt](mailto:root@hadoop103:/user/hep/hello.txt  hello.txt)      // 拉 pull</span><br><span class="line"></span><br><span class="line">​    scp -r [root@hadoop103:/user/hep/hello.txt](mailto:root@hadoop103:/user/hep/hello.txt) root@hadoop104:/user/hep  //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。</span><br></pre></td></tr></table></figure>

<h4 id="2．采用distcp命令实现两个Hadoop集群之间的递归数据复制"><a href="#2．采用distcp命令实现两个Hadoop集群之间的递归数据复制" class="headerlink" title="2．采用distcp命令实现两个Hadoop集群之间的递归数据复制"></a>2．采用distcp命令实现两个Hadoop集群之间的递归数据复制</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp</span><br><span class="line"></span><br><span class="line">hdfs://haoop102:9000/user/hep/hello.txt hdfs://hadoop103:9000/user/hep/hello.txt</span><br></pre></td></tr></table></figure>

<h3 id="7-2-小文件存档"><a href="#7-2-小文件存档" class="headerlink" title="7.2 小文件存档"></a>7.2 小文件存档</h3><h4 id="1、HDFS存储小文件弊端"><a href="#1、HDFS存储小文件弊端" class="headerlink" title="1、HDFS存储小文件弊端"></a>1、HDFS存储小文件弊端</h4><p>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。 </p>
<h4 id="2、解决存储小文件办法之一"><a href="#2、解决存储小文件办法之一" class="headerlink" title="2、解决存储小文件办法之一"></a>2、解决存储小文件办法之一</h4><p>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存。</p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/24.png" alt></p>
<h4 id="3．案例实操"><a href="#3．案例实操" class="headerlink" title="3．案例实操"></a>3．案例实操</h4><p><strong>（1）需要启动YARN进程</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ start-yarn.sh</span><br></pre></td></tr></table></figure>

<p><strong>（2）归档文件</strong></p>
<p>​    把/user/hep/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/hep/output路径下。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName input.har –p /user/hep/input  /user/hep/output</span><br></pre></td></tr></table></figure>

<p><strong>（3）查看归档</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/hep/output/input.har</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/hep/output/input.har</span><br></pre></td></tr></table></figure>

<p><strong>（4）解归档文件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/hep/output/input.har/*  /user/hep</span><br></pre></td></tr></table></figure>

<p>[TOC]</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>个人学习笔记</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>用虚拟机搭建一个小的集群</title>
    <url>/2020/01/07/%E7%94%A8%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%9A%84%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p><strong>前言：</strong>学习大数据的过程中，完全分布式运行模式至关重要。本篇将记录在使用虚拟机搭建一个小的集群的过程中需要使用到的linux命令。</p>
<h2 id="一、linux关闭防火墙的命令"><a href="#一、linux关闭防火墙的命令" class="headerlink" title="一、linux关闭防火墙的命令"></a>一、linux关闭防火墙的命令</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo service iptables stop</span><br><span class="line">sudo chkconfig iptables off</span><br></pre></td></tr></table></figure>

<h2 id="二、设置静态IP，改主机名"><a href="#二、设置静态IP，改主机名" class="headerlink" title="二、设置静态IP，改主机名"></a>二、设置静态IP，改主机名</h2><p><strong>vim /etc/sysconfig/network-scripts/ifcfg-eth0</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DEVICE=eth0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">NAME="eth0"</span><br><span class="line">IPADDR=192.168.5.101</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=192.168.5.2</span><br><span class="line">DNS1=192.168.5.2</span><br></pre></td></tr></table></figure>

<p><strong>vim /etc/sysconfig/network</strong><br>改HOSTNAME=那一行</p>
<a id="more"></a>

<h2 id="三、配置-etc-hosts"><a href="#三、配置-etc-hosts" class="headerlink" title="三、配置/etc/hosts"></a>三、配置/etc/hosts</h2><p><strong>vim /etc/hosts</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">192.168.1.100  hadoop100</span><br><span class="line">192.168.1.101  hadoop101</span><br><span class="line">192.168.1.102  hadoop102</span><br><span class="line">192.168.1.103  hadoop103</span><br><span class="line">192.168.1.104  hadoop104</span><br></pre></td></tr></table></figure>

<h2 id="四、创建一般用户，配置密码"><a href="#四、创建一般用户，配置密码" class="headerlink" title="四、创建一般用户，配置密码"></a>四、创建一般用户，配置密码</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">useradd hep</span><br><span class="line">passwd 123456</span><br></pre></td></tr></table></figure>
<p>(此处用户名为：hep 密码：123456)</p>
<h2 id="五、配置用户为sudoers"><a href="#五、配置用户为sudoers" class="headerlink" title="五、配置用户为sudoers"></a>五、配置用户为sudoers</h2><p>​    <strong>vim /etc/sudoers</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root    ALL=(ALL)       ALL</span><br><span class="line">hep     ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure>
<p>​    保存时wq!强制保存</p>
<h2 id="六、在-opt目录下创建两个文件夹modulesoftware"><a href="#六、在-opt目录下创建两个文件夹modulesoftware" class="headerlink" title="六、在/opt目录下创建两个文件夹modulesoftware"></a>六、在/opt目录下创建两个文件夹modulesoftware</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir /opt/module /opt/software</span><br><span class="line">chown hep:hep /opt/module /opt/software</span><br></pre></td></tr></table></figure>

<h2 id="七、写一个分发脚本xsync"><a href="#七、写一个分发脚本xsync" class="headerlink" title="七、写一个分发脚本xsync"></a>七、写一个分发脚本xsync</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd ~</span><br><span class="line">vim xsync</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">=================================================</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if ((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=103; host&lt;105; host++)); do</span><br><span class="line">     echo --------- hadoop$host ----------</span><br><span class="line">        rsync -av $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br><span class="line">=================================================</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x xsync</span><br><span class="line">sudo cp xsync /bin</span><br><span class="line">sudo xsync /bin/xsync</span><br></pre></td></tr></table></figure>
<h2 id="八、配置免密登陆"><a href="#八、配置免密登陆" class="headerlink" title="八、配置免密登陆"></a>八、配置免密登陆</h2><p><img src="/2020/01/07/%E7%94%A8%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%9A%84%E9%9B%86%E7%BE%A4/01.png" alt></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.生成密钥对</span><br><span class="line">ssh-keygen -t rsa 三次回车</span><br><span class="line">2. 发送公钥到本机</span><br><span class="line">ssh-copy-id hadoop102 输入一次密码</span><br><span class="line">3. 分别ssh登陆一下所有虚拟机</span><br><span class="line">ssh hadoop103</span><br><span class="line">exit</span><br><span class="line">ssh hadoop104</span><br><span class="line">exit</span><br><span class="line">4. 把/home/atguigu/.ssh 文件夹发送到集群所有服务器</span><br><span class="line">xsync /home/atguigu/.ssh</span><br></pre></td></tr></table></figure>
<h2 id="九、在一台机器上安装Java和Hadoop，并配置环境变量，并分发到集群其他机器"><a href="#九、在一台机器上安装Java和Hadoop，并配置环境变量，并分发到集群其他机器" class="headerlink" title="九、在一台机器上安装Java和Hadoop，并配置环境变量，并分发到集群其他机器"></a>九、在一台机器上安装Java和Hadoop，并配置环境变量，并分发到集群其他机器</h2><h3 id="1-拷贝文件到-opt-software，两个tar包"><a href="#1-拷贝文件到-opt-software，两个tar包" class="headerlink" title="1.拷贝文件到/opt/software，两个tar包"></a>1.拷贝文件到/opt/software，两个tar包</h3><h3 id="2-tar-zxf-h”-tab”-C-opt-module"><a href="#2-tar-zxf-h”-tab”-C-opt-module" class="headerlink" title="2.tar -zxf h”+tab” -C /opt/module"></a>2.tar -zxf h”+tab” -C /opt/module</h3><h3 id="3-tar-zxf-j”-tab”-C-opt-module"><a href="#3-tar-zxf-j”-tab”-C-opt-module" class="headerlink" title="3.tar -zxf j”+tab” -C /opt/module"></a>3.tar -zxf j”+tab” -C /opt/module</h3><h3 id="4-sudo-vim-etc-profile"><a href="#4-sudo-vim-etc-profile" class="headerlink" title="4.sudo vim /etc/profile"></a>4.sudo vim /etc/profile</h3><p>在文件末尾添加</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>
<h3 id="5-source-etc-profile"><a href="#5-source-etc-profile" class="headerlink" title="5.source /etc/profile"></a>5.source /etc/profile</h3><h3 id="6-sudo-xsync-etc-profile"><a href="#6-sudo-xsync-etc-profile" class="headerlink" title="6.sudo xsync /etc/profile"></a>6.sudo xsync /etc/profile</h3><h3 id="7-在其他机器分别执行source-etc-profile"><a href="#7-在其他机器分别执行source-etc-profile" class="headerlink" title="7.在其他机器分别执行source /etc/profile"></a>7.在其他机器分别执行source /etc/profile</h3><p>所有配置文件都在$HADOOP_HOME/etc/hadoop</p>
<h2 id="十、首先配置hadoop-env-sh-yarn-env-sh-mapred-env-sh文件"><a href="#十、首先配置hadoop-env-sh-yarn-env-sh-mapred-env-sh文件" class="headerlink" title="十、首先配置hadoop-env.sh,yarn-env.sh,mapred-env.sh文件"></a>十、首先配置hadoop-env.sh,yarn-env.sh,mapred-env.sh文件</h2><p>配置JAVA_HOME,在每个文件第二行添加</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>

<h2 id="十一、配置Core-site-xml"><a href="#十一、配置Core-site-xml" class="headerlink" title="十一、配置Core-site.xml"></a>十一、配置Core-site.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="十二、配置hdfs-site-xml"><a href="#十二、配置hdfs-site-xml" class="headerlink" title="十二、配置hdfs-site.xml"></a>十二、配置hdfs-site.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 数据的副本数量 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="十三、配置yarn-site-xml"><a href="#十三、配置yarn-site-xml" class="headerlink" title="十三、配置yarn-site.xml"></a>十三、配置yarn-site.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="十四、配置mapred-site-xml"><a href="#十四、配置mapred-site-xml" class="headerlink" title="十四、配置mapred-site.xml"></a>十四、配置mapred-site.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>启动历史服务器：mr-jobhistory-daemon.sh start historyserver</p>
<h2 id="十五、配置Slaves"><a href="#十五、配置Slaves" class="headerlink" title="十五、配置Slaves"></a>十五、配置Slaves</h2><p>hadoop102<br>hadoop103<br>hadoop104</p>
<h2 id="十六、分发配置文件"><a href="#十六、分发配置文件" class="headerlink" title="十六、分发配置文件"></a>十六、分发配置文件</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop-2.7.2/etc</span><br></pre></td></tr></table></figure>

<h2 id="十七、格式化Namenode-在hadoop102"><a href="#十七、格式化Namenode-在hadoop102" class="headerlink" title="十七、格式化Namenode 在hadoop102"></a>十七、格式化Namenode 在hadoop102</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h2 id="十八、启动-停止hdfs"><a href="#十八、启动-停止hdfs" class="headerlink" title="十八、启动/停止hdfs"></a>十八、启动/停止hdfs</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-dfs.sh </span><br><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure>

<h2 id="十九、启动-停止yarn"><a href="#十九、启动-停止yarn" class="headerlink" title="十九、启动/停止yarn"></a>十九、启动/停止yarn</h2><p>在配置了Resourcemanager机器上执行<br>在Hadoop103上启动start-yarn.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start-yarn.sh </span><br><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure>

<p>jps查看启动的集群各机器工作状态</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>个人学习笔记</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title>从Hadoop框架学习大数据生态</title>
    <url>/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/</url>
    <content><![CDATA[<p><strong>前言：</strong>现如今，大数据已经在各个领域运用广泛，具有Volume、Velocity、Variety、Value（即大量、高速、多样以及低价值密度）的特点，本文谨记录本人通过Hadoop框架学习大数据生态的过程以及如何在虚拟机上搭建一个简单的Hadoop运行环境，本文仅用于交流学习。图片以及资料来自于各大视频网站以及论坛，如出现表述错误的地方还请各位大佬指正~</p>
<h2 id="一、初识Hadoop"><a href="#一、初识Hadoop" class="headerlink" title="一、初识Hadoop"></a>一、初识Hadoop</h2><h3 id="1-什么是Hadoop"><a href="#1-什么是Hadoop" class="headerlink" title="1.什么是Hadoop"></a>1.什么是Hadoop</h3><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop01.png" alt></p>
<h3 id="2-Hadoop发展历史"><a href="#2-Hadoop发展历史" class="headerlink" title="2.Hadoop发展历史"></a>2.Hadoop发展历史</h3><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop02.png" alt></p>
<a id="more"></a>

<h3 id="3-Hadoop的优势（4高）"><a href="#3-Hadoop的优势（4高）" class="headerlink" title="3.Hadoop的优势（4高）"></a>3.Hadoop的优势（4高）</h3><p>（1）高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</p>
<p>（2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。</p>
<p>（3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</p>
<p>（4）高容错性：能够自动将失败的任务重新分配。</p>
<h3 id="4-Hadoop组成"><a href="#4-Hadoop组成" class="headerlink" title="4.Hadoop组成"></a>4.Hadoop组成</h3><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop03.png" alt></p>
<h4 id="（1-HDFS架构概述"><a href="#（1-HDFS架构概述" class="headerlink" title="（1)HDFS架构概述"></a>（1)HDFS架构概述</h4><ul>
<li><strong>NameNode（nn）</strong>：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。</li>
<li><strong>Data Node(dn)</strong>：在本地文件系统存储文件块数据，以及块数据的校验和。</li>
<li><strong>Secondary NameNode(2nn)</strong>：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。</li>
</ul>
<h4 id="（2-YARN架构概述"><a href="#（2-YARN架构概述" class="headerlink" title="（2)YARN架构概述"></a>（2)YARN架构概述</h4><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop04.png" alt></p>
<h4 id="（3）MapReduce架构概述"><a href="#（3）MapReduce架构概述" class="headerlink" title="（3）MapReduce架构概述"></a>（3）MapReduce架构概述</h4><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop05.png" alt></p>
<h2 id="二、大数据技术生态体系"><a href="#二、大数据技术生态体系" class="headerlink" title="二、大数据技术生态体系"></a>二、大数据技术生态体系</h2><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop06.png" alt></p>
<p><strong>图中涉及的技术名词解释如下：</strong></p>
<ul>
<li><p><strong>Sqoop：</strong>Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
</li>
<li><p><strong>Flume：</strong>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</p>
</li>
<li><p><strong>Kafka：</strong>Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：</p>
<p>（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。</p>
<p>（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。</p>
<p>（3）支持通过Kafka服务器和消费机集群来分区消息。</p>
<p>（4）支持Hadoop并行数据加载。</p>
</li>
<li><p><strong>Storm：</strong>Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。</p>
</li>
<li><p><strong>Spark：</strong>Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p>
</li>
<li><p><strong>Oozie：</strong>Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。</p>
</li>
<li><p><strong>Hbase：</strong>HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p>
</li>
<li><p><strong>Hive：</strong>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
</li>
<li><p><strong>R语言</strong>：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。</p>
</li>
<li><p><strong>Mahout：</strong>Apache Mahout是个可扩展的机器学习和数据挖掘库。</p>
</li>
<li><p><strong>ZooKeeper：</strong>Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>
</li>
</ul>
<h2 id="三、Hadoop运行环境搭建"><a href="#三、Hadoop运行环境搭建" class="headerlink" title="三、Hadoop运行环境搭建"></a>三、Hadoop运行环境搭建</h2><h3 id="1-Hadoop下载"><a href="#1-Hadoop下载" class="headerlink" title="1.Hadoop下载"></a>1.Hadoop下载</h3><p>Hadoop下载地址：<a href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/" target="_blank" rel="noopener">https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/</a></p>
<p>由于我是运行在虚拟机（CentOs）上，故用XShell或SecureCRT工具将hadoop-2.7.2.tar.gz导入到opt目录下面的software文件夹下面。</p>
<h3 id="2-进入到Hadoop安装包路径下"><a href="#2-进入到Hadoop安装包路径下" class="headerlink" title="2.进入到Hadoop安装包路径下"></a>2.进入到Hadoop安装包路径下</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 ~]$ cd /opt/software/</span><br></pre></td></tr></table></figure>

<h3 id="3-解压安装文件到-opt-module下面"><a href="#3-解压安装文件到-opt-module下面" class="headerlink" title="3.解压安装文件到/opt/module下面"></a>3.解压安装文件到/opt/module下面</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 software]$ tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<h3 id="4-查看是否解压成功"><a href="#4-查看是否解压成功" class="headerlink" title="4.查看是否解压成功"></a>4.查看是否解压成功</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 software]$ ls /opt/module/</span><br><span class="line">hadoop-2.7.2</span><br></pre></td></tr></table></figure>

<h3 id="5-将Hadoop添加到环境变量"><a href="#5-将Hadoop添加到环境变量" class="headerlink" title="5.将Hadoop添加到环境变量"></a>5.将Hadoop添加到环境变量</h3><h4 id="1-获取Hadoop安装路径"><a href="#1-获取Hadoop安装路径" class="headerlink" title="(1)获取Hadoop安装路径"></a>(1)获取Hadoop安装路径</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 hadoop-2.7.2]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure>

<h4 id="2-打开-etc-profile文件"><a href="#2-打开-etc-profile文件" class="headerlink" title="(2)打开/etc/profile文件"></a>(2)打开/etc/profile文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 hadoop-2.7.2]$ sudo vim /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="3-在profile文件末尾添加JDK路径"><a href="#3-在profile文件末尾添加JDK路径" class="headerlink" title="(3)在profile文件末尾添加JDK路径"></a>(3)在profile文件末尾添加JDK路径</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#HADOOP_HOME</span></span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<h4 id="4-让修改后的文件生效"><a href="#4-让修改后的文件生效" class="headerlink" title="(4)让修改后的文件生效"></a>(4)让修改后的文件生效</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@ hadoop101 hadoop-2.7.2]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="5-测试是否安装成功"><a href="#5-测试是否安装成功" class="headerlink" title="(5)测试是否安装成功"></a>(5)测试是否安装成功</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 hadoop-2.7.2]$ hadoop version</span><br><span class="line">Hadoop 2.7.2</span><br></pre></td></tr></table></figure>

<h3 id="6-了解Hadoop的目录结构"><a href="#6-了解Hadoop的目录结构" class="headerlink" title="6.了解Hadoop的目录结构"></a>6.了解Hadoop的目录结构</h3><ul>
<li><p><strong>bin目录：</strong>存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本</p>
</li>
<li><p><strong>etc目录：</strong>Hadoop的配置文件目录，存放Hadoop的配置文件</p>
</li>
<li><p><strong>lib目录：</strong>存放Hadoop的本地库（对数据进行压缩解压缩功能）</p>
</li>
<li><p><strong>sbin目录：</strong>存放启动或停止Hadoop相关服务的脚本</p>
</li>
<li><p><strong>share目录：</strong>存放Hadoop的依赖jar包、文档、和官方案例</p>
</li>
</ul>
<h2 id="四、结语"><a href="#四、结语" class="headerlink" title="四、结语"></a>四、结语</h2><p>至此，已经粗略通过对Hadoop框架的学习大致了解了大数据的生态，Hadoop的环境也已经搭好。接下来将对hdfs、yarn以及mapreduce进行深入的了解。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>个人学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown笔记</title>
    <url>/2019/12/31/Markdown%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><strong>前言：</strong>Markdown一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。本文主要记录Markdown的一些基本语法。</p>
<h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h2><h3 id="1-语法格式"><a href="#1-语法格式" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section">#(空格)一级标题</span></span><br><span class="line"><span class="section">##(空格)二级标题</span></span><br><span class="line"><span class="section">###(空格)三级标题</span></span><br><span class="line"><span class="section">####(空格)四级标题</span></span><br><span class="line"><span class="section">#####(空格)五级标题</span></span><br><span class="line"><span class="section">######(空格)六级标题</span></span><br></pre></td></tr></table></figure>

<h3 id="2-效果图"><a href="#2-效果图" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><img src="/2019/12/31/Markdown%E7%AC%94%E8%AE%B0/markdown_01.png" style="zoom:50%;">

<a id="more"></a>

<h2 id="2-字体样式"><a href="#2-字体样式" class="headerlink" title="2.字体样式"></a>2.字体样式</h2><h3 id="1-语法格式-1"><a href="#1-语法格式-1" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="strong">**加粗的语言**</span></span><br><span class="line">~~被删除的文字~~</span><br><span class="line"><span class="emphasis">*斜体内容*</span></span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-1"><a href="#2-效果图-1" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><p><strong>加粗的语言</strong><br><del>被删除的文字</del><br><em>斜体内容*</em></p>
<h2 id="3-引用"><a href="#3-引用" class="headerlink" title="3.引用"></a>3.引用</h2><h3 id="1-语法格式-2"><a href="#1-语法格式-2" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&gt;一级引用</span><br><span class="line">&gt;&gt;二级引用</span><br><span class="line">&gt;&gt;&gt;三级引用</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-2"><a href="#2-效果图-2" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><blockquote>
<p>一级引用</p>
<blockquote>
<p>二级引用</p>
<blockquote>
<p>三级引用</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="4-分割线"><a href="#4-分割线" class="headerlink" title="4.分割线"></a>4.分割线</h2><h3 id="1-语法格式-3"><a href="#1-语法格式-3" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">---分割线1</span><br><span class="line"><span class="emphasis">***</span>分割线2</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-3"><a href="#2-效果图-3" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><hr>
<hr>
<h2 id="5-图片插入"><a href="#5-图片插入" class="headerlink" title="5.图片插入"></a>5.图片插入</h2><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">!(我的照片)[图片路径]</span><br></pre></td></tr></table></figure>

<p><img src="/2019/12/31/Markdown%E7%AC%94%E8%AE%B0/markdown_02.png" alt></p>
<h2 id="6-超链接"><a href="#6-超链接" class="headerlink" title="6.超链接"></a>6.超链接</h2><h3 id="1-语法格式-4"><a href="#1-语法格式-4" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">[链接名称]（链接地址）</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-4"><a href="#2-效果图-4" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><p><a href="https://www.baidu.com" target="_blank" rel="noopener">百度一下</a></p>
<h2 id="7-代码块"><a href="#7-代码块" class="headerlink" title="7.代码块"></a>7.代码块</h2><h3 id="1-语法格式-5"><a href="#1-语法格式-5" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">​<span class="code">```</span>代码语言（java）</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-5"><a href="#2-效果图-5" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.math.BigDecimal;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestBody;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.api.EventApi;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.service.EventService;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.common.base.util.BeanUtil;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.vo.EventVo;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.vo.ResponseVo;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.vo.SelectValueVo;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.common.base.vo.PageVo;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventController</span> <span class="keyword">implements</span> <span class="title">EventApi</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Autowired</span></span><br><span class="line">	EventService eventService;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> PageVo&lt;EventVo&gt; <span class="title">queryEvent</span><span class="params">(@RequestBody(required=<span class="keyword">false</span>)</span> EventVo vo)</span>&#123;</span><br><span class="line">		List&lt;EventVo&gt; list = eventService.queryEvent(vo);</span><br><span class="line">		<span class="keyword">return</span> BeanUtil.page(list);	</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">addEvent</span><span class="params">(@RequestBody EventVo vo)</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> eventService.addEvent(vo);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> EventVo <span class="title">findEventById</span><span class="params">(@RequestParam(<span class="string">"id"</span>)</span> Long id)</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> eventService.findEventById(id);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">updateEventById</span><span class="params">(@RequestBody EventVo vo)</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> eventService.updateEventById(vo);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">delEventById</span><span class="params">(@RequestParam(<span class="string">"id"</span>)</span> Long id)</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> eventService.delEventById(id);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<h2 id="8-列表"><a href="#8-列表" class="headerlink" title="8.列表"></a>8.列表</h2><h3 id="1-语法格式-6"><a href="#1-语法格式-6" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">-(空格)无序列表目录1</span><br><span class="line"> -(空格)无序列表目录2</span><br><span class="line">  -(空格)无序列表目录3</span><br><span class="line"></span><br><span class="line">1.有序列表目录1</span><br><span class="line">2.有序列表目录2</span><br><span class="line">3.有序列表目录3</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-6"><a href="#2-效果图-6" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><ul>
<li>无序列表目录1<ul>
<li>无序列表目录2<ul>
<li>无序列表目录3</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>1.有序列表目录1<br>2.有序列表目录2<br>3.有序列表目录3</p>
<h2 id="9-表格"><a href="#9-表格" class="headerlink" title="9.表格"></a>9.表格</h2><h3 id="1-语法格式-7"><a href="#1-语法格式-7" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">| 姓名 | 学号 | 成绩 |</span><br><span class="line">| :--: | :--: | :--: |</span><br><span class="line">|      |      |      |</span><br><span class="line">|      |      |      |</span><br><span class="line">|      |      |      |</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-7"><a href="#2-效果图-7" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><table>
<thead>
<tr>
<th align="center">姓名</th>
<th align="center">学号</th>
<th align="center">成绩</th>
</tr>
</thead>
<tbody><tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>个人学习笔记</tag>
        <tag>Markdown</tag>
        <tag>基本语法</tag>
      </tags>
  </entry>
</search>
