<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hadoop之HDFS详解</title>
    <url>/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="第1章-HDFS概述"><a href="#第1章-HDFS概述" class="headerlink" title="第1章 HDFS概述"></a>第1章 HDFS概述</h2><h3 id="1-1-HDFS产生背景"><a href="#1-1-HDFS产生背景" class="headerlink" title="1.1 HDFS产生背景"></a>1.1 HDFS产生背景</h3><p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。</p>
<h3 id="1-2HDFS的定义"><a href="#1-2HDFS的定义" class="headerlink" title="1.2HDFS的定义"></a>1.2HDFS的定义</h3><p>HDFS（Hadoop Distributed File System），它是一个<strong>文件系统</strong>，用于存储文件，通过目录树来定位文件；<strong>其次，它是分布式的</strong>，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<br><strong>HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。</strong>适合用来做数据分析，并不适合用来做网盘应用。</p>
<h3 id="1-3HDFS优缺点"><a href="#1-3HDFS优缺点" class="headerlink" title="1.3HDFS优缺点"></a>1.3HDFS优缺点</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/01.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/E:%5Cblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3%5C02.png" alt></p>
<h3 id="1-4-HDFS组成架构"><a href="#1-4-HDFS组成架构" class="headerlink" title="1.4 HDFS组成架构"></a>1.4 HDFS组成架构</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/03.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/04.png" alt></p>
<h3 id="1-5-HDFS文件块大小"><a href="#1-5-HDFS文件块大小" class="headerlink" title="1.5 HDFS文件块大小"></a>1.5 HDFS文件块大小</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/05.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/06.png" alt></p>
<h2 id="第2章-HDFS的Shell操作"><a href="#第2章-HDFS的Shell操作" class="headerlink" title="第2章 HDFS的Shell操作"></a>第2章 HDFS的Shell操作</h2><h3 id="2-1基本语法"><a href="#2-1基本语法" class="headerlink" title="2.1基本语法"></a>2.1基本语法</h3><p><strong>bin/hadoop fs</strong> 具体命令  或者 <strong>bin/hdfs dfs</strong> 具体命令</p>
<h3 id="2-2常用命令实操"><a href="#2-2常用命令实操" class="headerlink" title="2.2常用命令实操"></a>2.2常用命令实操</h3><h4 id="（0）启动Hadoop集群"><a href="#（0）启动Hadoop集群" class="headerlink" title="（0）启动Hadoop集群"></a>（0）启动Hadoop集群</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[hep@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="（1）-help：输出这个命令参数"><a href="#（1）-help：输出这个命令参数" class="headerlink" title="（1）-help：输出这个命令参数"></a>（1）-help：输出这个命令参数</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm</span><br></pre></td></tr></table></figure>

<h4 id="（2）-ls-显示目录信息"><a href="#（2）-ls-显示目录信息" class="headerlink" title="（2）-ls: 显示目录信息"></a>（2）-ls: 显示目录信息</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /</span><br></pre></td></tr></table></figure>

<h4 id="（3）-mkdir：在HDFS上创建目录"><a href="#（3）-mkdir：在HDFS上创建目录" class="headerlink" title="（3）-mkdir：在HDFS上创建目录"></a>（3）-mkdir：在HDFS上创建目录</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<h4 id="（4）-moveFromLocal：从本地剪切粘贴到HDFS"><a href="#（4）-moveFromLocal：从本地剪切粘贴到HDFS" class="headerlink" title="（4）-moveFromLocal：从本地剪切粘贴到HDFS"></a>（4）-moveFromLocal：从本地剪切粘贴到HDFS</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch kongming.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<h4 id="（5）-appendToFile：追加一个文件到已经存在的文件末尾"><a href="#（5）-appendToFile：追加一个文件到已经存在的文件末尾" class="headerlink" title="（5）-appendToFile：追加一个文件到已经存在的文件末尾"></a>（5）-appendToFile：追加一个文件到已经存在的文件末尾</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch liubei.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ vi liubei.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（6）-cat：显示文件内容"><a href="#（6）-cat：显示文件内容" class="headerlink" title="（6）-cat：显示文件内容"></a>（6）-cat：显示文件内容</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（7）-chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"><a href="#（7）-chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限" class="headerlink" title="（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"></a>（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -chown atguigu:atguigu  /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"><a href="#（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去" class="headerlink" title="（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"></a>（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /</span><br></pre></td></tr></table></figure>

<h4 id="（9）-copyToLocal：从HDFS拷贝到本地"><a href="#（9）-copyToLocal：从HDFS拷贝到本地" class="headerlink" title="（9）-copyToLocal：从HDFS拷贝到本地"></a>（9）-copyToLocal：从HDFS拷贝到本地</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>

<h4 id="（10）-cp-：从HDFS的一个路径拷贝到HDFS的另一个路径"><a href="#（10）-cp-：从HDFS的一个路径拷贝到HDFS的另一个路径" class="headerlink" title="（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径"></a>（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt</span><br></pre></td></tr></table></figure>

<h4 id="（11）-mv：在HDFS目录中移动文件"><a href="#（11）-mv：在HDFS目录中移动文件" class="headerlink" title="（11）-mv：在HDFS目录中移动文件"></a>（11）-mv：在HDFS目录中移动文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/</span><br></pre></td></tr></table></figure>

<h4 id="（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地"><a href="#（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地" class="headerlink" title="（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地"></a>（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>

<h4 id="（13）-getmerge：合并下载多个文件"><a href="#（13）-getmerge：合并下载多个文件" class="headerlink" title="（13）-getmerge：合并下载多个文件"></a>（13）-getmerge：合并下载多个文件</h4><p>比如HDFS的目录 /user/hep/test下有多个文件:log.1, log.2,log.3,…</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/hep/test/* ./zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<h4 id="（14）-put：等同于copyFromLocal"><a href="#（14）-put：等同于copyFromLocal" class="headerlink" title="（14）-put：等同于copyFromLocal"></a>（14）-put：等同于copyFromLocal</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/hep/test/</span><br></pre></td></tr></table></figure>

<h4 id="（15）-tail：显示一个文件的末尾"><a href="#（15）-tail：显示一个文件的末尾" class="headerlink" title="（15）-tail：显示一个文件的末尾"></a>（15）-tail：显示一个文件的末尾</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（16）-rm：删除文件或文件夹"><a href="#（16）-rm：删除文件或文件夹" class="headerlink" title="（16）-rm：删除文件或文件夹"></a>（16）-rm：删除文件或文件夹</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/hep/test/jinlian2.txt</span><br></pre></td></tr></table></figure>

<h4 id="（17）-rmdir：删除空目录"><a href="#（17）-rmdir：删除空目录" class="headerlink" title="（17）-rmdir：删除空目录"></a>（17）-rmdir：删除空目录</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test</span><br></pre></td></tr></table></figure>

<h4 id="（18）-du统计文件夹的大小信息"><a href="#（18）-du统计文件夹的大小信息" class="headerlink" title="（18）-du统计文件夹的大小信息"></a>（18）-du统计文件夹的大小信息</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/hep/test</span><br></pre></td></tr></table></figure>

<p><strong>结果：</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2.7 K /user/atguigu/test</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/hep/test</span><br></pre></td></tr></table></figure>

<p><strong>结果：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.3 K &#x2F;user&#x2F;hep&#x2F;test&#x2F;README.txt</span><br><span class="line">15   &#x2F;user&#x2F;hep&#x2F;test&#x2F;jinlian.txt</span><br><span class="line">1.4 K &#x2F;user&#x2F;hep&#x2F;test&#x2F;zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<h4 id="（19）-setrep：设置HDFS中文件的副本数量"><a href="#（19）-setrep：设置HDFS中文件的副本数量" class="headerlink" title="（19）-setrep：设置HDFS中文件的副本数量"></a>（19）-setrep：设置HDFS中文件的副本数量</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 &#x2F;sanguo&#x2F;shuguo&#x2F;kongming.txt</span><br></pre></td></tr></table></figure>

<p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
<h2 id="第3章-HDFS客户端操作"><a href="#第3章-HDFS客户端操作" class="headerlink" title="第3章 HDFS客户端操作"></a>第3章 HDFS客户端操作</h2><h3 id="3-1-HDFS客户端环境准备"><a href="#3-1-HDFS客户端环境准备" class="headerlink" title="3.1 HDFS客户端环境准备"></a>3.1 HDFS客户端环境准备</h3><h4 id="1．拷贝对应的编译后的hadoop-jar包到非中文路径"><a href="#1．拷贝对应的编译后的hadoop-jar包到非中文路径" class="headerlink" title="1．拷贝对应的编译后的hadoop jar包到非中文路径"></a>1．拷贝对应的编译后的hadoop jar包到非中文路径</h4><h4 id="2．配置HADOOP-HOME环境变量"><a href="#2．配置HADOOP-HOME环境变量" class="headerlink" title="2．配置HADOOP_HOME环境变量"></a>2．配置HADOOP_HOME环境变量</h4><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/07.png" alt></p>
<h4 id="3-配置Path环境变量"><a href="#3-配置Path环境变量" class="headerlink" title="3.配置Path环境变量"></a>3.配置Path环境变量</h4><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/08.png" alt></p>
<h4 id="4-执行winutils判断是否配置成功"><a href="#4-执行winutils判断是否配置成功" class="headerlink" title="4.执行winutils判断是否配置成功"></a>4.执行winutils判断是否配置成功</h4><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/09.png" alt></p>
<h4 id="5-创建一个Maven工程HdfsClient"><a href="#5-创建一个Maven工程HdfsClient" class="headerlink" title="5.创建一个Maven工程HdfsClient"></a>5.创建一个Maven工程HdfsClient</h4><h4 id="6-导入相应的依赖坐标-日志添加"><a href="#6-导入相应的依赖坐标-日志添加" class="headerlink" title="6.导入相应的依赖坐标+日志添加"></a>6.导入相应的依赖坐标+日志添加</h4><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：如果Eclipse/Idea打印不出日志，在控制台上只显示</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">1.log4j</span>:<span class="string">WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). </span></span><br><span class="line"><span class="meta">2.log4j</span>:<span class="string">WARN Please initialize the log4j system properly. </span></span><br><span class="line"><span class="meta">3.log4j</span>:<span class="string">WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span></span><br></pre></td></tr></table></figure>

<p><strong>需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</strong></p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<h4 id="7-创建包名：com-hep-hdfs"><a href="#7-创建包名：com-hep-hdfs" class="headerlink" title="7.创建包名：com.hep.hdfs"></a>7.创建包名：com.hep.hdfs</h4><h4 id="8-创建HdfsClient类"><a href="#8-创建HdfsClient类" class="headerlink" title="8.创建HdfsClient类"></a>8.创建HdfsClient类</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span>&#123;  </span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line">   <span class="comment">// 1 获取文件系统</span></span><br><span class="line">   Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">   <span class="comment">// 配置在集群上运行</span></span><br><span class="line">   <span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span></span><br><span class="line">   <span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line">   FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line">   <span class="comment">// 2 创建目录</span></span><br><span class="line">   fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/new"</span>));</span><br><span class="line">   <span class="comment">// 3 关闭资源</span></span><br><span class="line">   fs.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2HDFS的API操作"><a href="#3-2HDFS的API操作" class="headerlink" title="3.2HDFS的API操作"></a>3.2HDFS的API操作</h3><h4 id="1-HDFS文件下载"><a href="#1-HDFS文件下载" class="headerlink" title="1.HDFS文件下载"></a>1.HDFS文件下载</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//获取一个HDFS的抽象封装对象</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fileSystem = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line"><span class="comment">//用这个对象操作文件系统</span></span><br><span class="line"> fileSystem.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/test"</span>), <span class="keyword">new</span> Path(<span class="string">"e:\\"</span>));</span><br><span class="line"><span class="comment">//关闭文件系统</span></span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-HDFS文件名更改"><a href="#2-HDFS文件名更改" class="headerlink" title="2.HDFS文件名更改"></a>2.HDFS文件名更改</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">FileSystem fileSystem = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"hep"</span>);</span><br><span class="line">fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/test"</span>), <span class="keyword">new</span> Path(<span class="string">"/test2"</span>));</span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-HDFS文件删除"><a href="#3-HDFS文件删除" class="headerlink" title="3.HDFS文件删除"></a>3.HDFS文件删除</h4><p>此处因为每一次对文件系统进行操作前都需要获取一个HDFS的抽象封装对象，以及在使用完之后关闭文件系统。于是我在测试前写了个@Before和@After，便于后续的测试。之后的API测试均由生成的fs替代。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> FileSystem fs;</span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">before</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"hep"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Before!!!!!"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">after</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    s.close();</span><br><span class="line">    System.out.println(<span class="string">"After!!!!!!!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> delete = fs.delete(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">if</span> (delete) &#123;</span><br><span class="line">        System.out.println(<span class="string">"删除成功"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">"删除失败"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-HDFS文件的追加"><a href="#4-HDFS文件的追加" class="headerlink" title="4.HDFS文件的追加"></a>4.HDFS文件的追加</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    FSDataOutputStream append = fs.append(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>),<span class="number">1024</span>);</span><br><span class="line">    FileInputStream open = <span class="keyword">new</span> FileInputStream(<span class="string">"E://test/1.txt"</span>);</span><br><span class="line">    IOUtils.copyBytes(open,append,<span class="number">1024</span>,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-HDFS文件详情查看以及文件和文件夹判断"><a href="#5-HDFS文件详情查看以及文件和文件夹判断" class="headerlink" title="5.HDFS文件详情查看以及文件和文件夹判断"></a>5.HDFS文件详情查看以及文件和文件夹判断</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    <span class="keyword">for</span>(FileStatus fileStatus : fileStatuses)&#123;</span><br><span class="line">        <span class="keyword">if</span>(fileStatus.isFile())&#123;</span><br><span class="line">            System.out.println(<span class="string">"以下信息是一个文件的信息："</span>);</span><br><span class="line">            System.out.println(fileStatus.getPath());</span><br><span class="line">            System.out.println(fileStatus.getLen());</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"这是一个文件夹"</span>);</span><br><span class="line">            System.out.println(fileStatus.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="6-列出路径下所有的文件块信息"><a href="#6-列出路径下所有的文件块信息" class="headerlink" title="6.列出路径下所有的文件块信息"></a>6.列出路径下所有的文件块信息</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>),<span class="keyword">true</span>);<span class="comment">//此处的true是指递归深入文件夹中的文件夹(只能查文件，因为文件夹没有block)</span></span><br><span class="line">    <span class="keyword">while</span>(files.hasNext()) &#123;</span><br><span class="line">        LocatedFileStatus file = files.next();</span><br><span class="line">        System.out.println(<span class="string">"============================"</span>);</span><br><span class="line">        System.out.println(file.getPath());</span><br><span class="line">        System.out.println(<span class="string">"块信息："</span>);</span><br><span class="line">        BlockLocation[] blockLocations = file.getBlockLocations();</span><br><span class="line">        <span class="keyword">for</span>(BlockLocation blockLocation : blockLocations)&#123;</span><br><span class="line">            String[] hosts = blockLocation.getHosts();</span><br><span class="line">            System.out.print(<span class="string">"块在"</span>);</span><br><span class="line">            <span class="keyword">for</span> (String host : hosts)&#123;</span><br><span class="line">                System.out.println(host+<span class="string">" "</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="7-HDFS文件上传"><a href="#7-HDFS文件上传" class="headerlink" title="7.HDFS文件上传"></a>7.HDFS文件上传</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">()</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    configuration.setInt(<span class="string">"dfs.replication"</span>,<span class="number">1</span>);<span class="comment">//设置配置文件</span></span><br><span class="line">    fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line">    fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"e:/test/1.txt"</span>),<span class="keyword">new</span> Path(<span class="string">"/2.txt"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此处的configuration有自定的配置，若api里有配置文件的设置，api的优先级更高，若没有则看配置文件中的配置。</p>
<h2 id="第4章-HDFS的数据流"><a href="#第4章-HDFS的数据流" class="headerlink" title="第4章 HDFS的数据流"></a>第4章 HDFS的数据流</h2><h3 id="4-1-HDFS写数据流程"><a href="#4-1-HDFS写数据流程" class="headerlink" title="4.1 HDFS写数据流程"></a>4.1 HDFS写数据流程</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/10.png" alt></p>
<ol>
<li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</li>
<li>NameNode返回是否可以上传。</li>
<li>客户端请求第一个 Block上传到哪几个DataNode服务器上。</li>
<li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</li>
<li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</li>
<li>dn1、dn2、dn3逐级应答客户端。</li>
<li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
</ol>
<h3 id="4-2-HDFS读数据流程"><a href="#4-2-HDFS读数据流程" class="headerlink" title="4.2 HDFS读数据流程"></a>4.2 HDFS读数据流程</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/11.png" alt></p>
<ol>
<li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<h3 id="4-3网络拓扑"><a href="#4-3网络拓扑" class="headerlink" title="4.3网络拓扑"></a>4.3网络拓扑</h3><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。</p>
<p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/12.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/13.png" alt></p>
<h3 id="4-4机架感知（副本存储节点选择）"><a href="#4-4机架感知（副本存储节点选择）" class="headerlink" title="4.4机架感知（副本存储节点选择）"></a>4.4机架感知（副本存储节点选择）</h3><blockquote>
<p>For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack.</p>
</blockquote>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/14.png" alt></p>
<h2 id="第5章-NameNode和SecondaryNameNode"><a href="#第5章-NameNode和SecondaryNameNode" class="headerlink" title="第5章 NameNode和SecondaryNameNode"></a>第5章 NameNode和SecondaryNameNode</h2><h3 id="5-1-NN和2NN工作机制"><a href="#5-1-NN和2NN工作机制" class="headerlink" title="5.1 NN和2NN工作机制"></a>5.1 NN和2NN工作机制</h3><blockquote>
<p><strong>思考：NameNode中的元数据是存储在哪里的？</strong></p>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。<strong>因此产生在磁盘中备份元数据的FsImage。</strong></p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失<strong>。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。</strong>这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。<strong>因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</strong></p>
</blockquote>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/15.png" alt></p>
<h4 id="1-第一阶段：NameNode启动"><a href="#1-第一阶段：NameNode启动" class="headerlink" title="1.第一阶段：NameNode启动"></a>1.第一阶段：NameNode启动</h4><p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>（2）客户端对元数据进行增删改的请求。<br>（3）NameNode记录操作日志，更新滚动日志。<br>（4）NameNode在内存中对元数据进行增删改。</p>
<h4 id="2-第二阶段：Secondary-NameNode工作"><a href="#2-第二阶段：Secondary-NameNode工作" class="headerlink" title="2.第二阶段：Secondary NameNode工作"></a>2.第二阶段：Secondary NameNode工作</h4><p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。<br>（2）Secondary NameNode请求执行CheckPoint。<br>（3）NameNode滚动正在写的Edits日志。<br>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br>（6）生成新的镜像文件fsimage.chkpoint。<br>（7）拷贝fsimage.chkpoint到NameNode。<br>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<hr>
<blockquote>
<p><strong>NN和2NN工作机制详解：</strong></p>
<p><strong>Fsimage：</strong>NameNode内存中元数据序列化后形成的文件。</p>
<p><strong>Edits：</strong>记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</p>
<p>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（<strong>所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage</strong>）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</p>
<p>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（<strong>触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了</strong>）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p>
</blockquote>
<h3 id="5-2-Fsimage和Edits解析"><a href="#5-2-Fsimage和Edits解析" class="headerlink" title="5.2 Fsimage和Edits解析"></a>5.2 Fsimage和Edits解析</h3><h4 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h4><p>​    NameNode被格式化之后，将在<strong>/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录</strong>中产生如下文件：<br><strong>（1）Fsimage文件：</strong>HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。<br><strong>（2）Edits文件：</strong>存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。<br><strong>（3）seen_txid文件：</strong>保存的是一个数字，就是最后一个edits_的数字<br>（4）每次NameNode<strong>启动的时候</strong>都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。</p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/16.png" alt></p>
<h4 id="2-oiv查看Fsimage文件"><a href="#2-oiv查看Fsimage文件" class="headerlink" title="2.oiv查看Fsimage文件"></a>2.oiv查看Fsimage文件</h4><h5 id="（1）查看oiv和oev命令"><a href="#（1）查看oiv和oev命令" class="headerlink" title="（1）查看oiv和oev命令"></a>（1）查看oiv和oev命令</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 current]$ hdfs</span><br><span class="line"></span><br><span class="line">**oiv**  apply the offline fsimage viewer to an fsimage</span><br><span class="line"></span><br><span class="line">**oev**  apply the offline edits viewer to an edits file</span><br></pre></td></tr></table></figure>

<h5 id="（2）基本语法"><a href="#（2）基本语法" class="headerlink" title="（2）基本语法"></a>（2）基本语法</h5><p><strong>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</strong></p>
<h5 id="（3）案例实操"><a href="#（3）案例实操" class="headerlink" title="（3）案例实操"></a>（3）案例实操</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop102 current]$  hdfs oiv -p XML -i fsimage_0000000000000000991 -o /opt/module/hadoop-2.7.2/fsimage.xml</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 current]$  hdfs oev -p XML -i edits_0000000000000000888-0000000000000000940 -o /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure>

<p>将显示的xml文件内容打开,部分显示结果如下。</p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/17.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/18.png" alt></p>
<h3 id="5-3-CheckPoint时间设置"><a href="#5-3-CheckPoint时间设置" class="headerlink" title="5.3 CheckPoint时间设置"></a>5.3 CheckPoint时间设置</h3><h4 id="1-通常情况下，SecondaryNameNode每隔一小时执行一次。"><a href="#1-通常情况下，SecondaryNameNode每隔一小时执行一次。" class="headerlink" title="1.通常情况下，SecondaryNameNode每隔一小时执行一次。"></a>1.通常情况下，SecondaryNameNode每隔一小时执行一次。</h4><p>​    <strong>[hdfs-default.xml]</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="2-自定义设置"><a href="#2-自定义设置" class="headerlink" title="2.自定义设置"></a>2.自定义设置</h4><p>一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>个人学习笔记</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>用虚拟机搭建一个小的集群</title>
    <url>/2020/01/07/%E7%94%A8%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%9A%84%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p><strong>前言：</strong>学习大数据的过程中，完全分布式运行模式至关重要。本篇将记录在使用虚拟机搭建一个小的集群的过程中需要使用到的linux命令。</p>
<h2 id="一、linux关闭防火墙的命令"><a href="#一、linux关闭防火墙的命令" class="headerlink" title="一、linux关闭防火墙的命令"></a>一、linux关闭防火墙的命令</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo service iptables stop</span><br><span class="line">sudo chkconfig iptables off</span><br></pre></td></tr></table></figure>

<h2 id="二、设置静态IP，改主机名"><a href="#二、设置静态IP，改主机名" class="headerlink" title="二、设置静态IP，改主机名"></a>二、设置静态IP，改主机名</h2><p><strong>vim /etc/sysconfig/network-scripts/ifcfg-eth0</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">DEVICE=eth0</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">NAME="eth0"</span><br><span class="line">IPADDR=192.168.5.101</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=192.168.5.2</span><br><span class="line">DNS1=192.168.5.2</span><br></pre></td></tr></table></figure>

<p><strong>vim /etc/sysconfig/network</strong><br>改HOSTNAME=那一行</p>
<a id="more"></a>

<h2 id="三、配置-etc-hosts"><a href="#三、配置-etc-hosts" class="headerlink" title="三、配置/etc/hosts"></a>三、配置/etc/hosts</h2><p><strong>vim /etc/hosts</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">192.168.1.100  hadoop100</span><br><span class="line">192.168.1.101  hadoop101</span><br><span class="line">192.168.1.102  hadoop102</span><br><span class="line">192.168.1.103  hadoop103</span><br><span class="line">192.168.1.104  hadoop104</span><br></pre></td></tr></table></figure>

<h2 id="四、创建一般用户，配置密码"><a href="#四、创建一般用户，配置密码" class="headerlink" title="四、创建一般用户，配置密码"></a>四、创建一般用户，配置密码</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">useradd hep</span><br><span class="line">passwd 123456</span><br></pre></td></tr></table></figure>
<p>(此处用户名为：hep 密码：123456)</p>
<h2 id="五、配置用户为sudoers"><a href="#五、配置用户为sudoers" class="headerlink" title="五、配置用户为sudoers"></a>五、配置用户为sudoers</h2><p>​    <strong>vim /etc/sudoers</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root    ALL=(ALL)       ALL</span><br><span class="line">hep     ALL=(ALL)       NOPASSWD:ALL</span><br></pre></td></tr></table></figure>
<p>​    保存时wq!强制保存</p>
<h2 id="六、在-opt目录下创建两个文件夹modulesoftware"><a href="#六、在-opt目录下创建两个文件夹modulesoftware" class="headerlink" title="六、在/opt目录下创建两个文件夹modulesoftware"></a>六、在/opt目录下创建两个文件夹modulesoftware</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir /opt/module /opt/software</span><br><span class="line">chown hep:hep /opt/module /opt/software</span><br></pre></td></tr></table></figure>

<h2 id="七、写一个分发脚本xsync"><a href="#七、写一个分发脚本xsync" class="headerlink" title="七、写一个分发脚本xsync"></a>七、写一个分发脚本xsync</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd ~</span><br><span class="line">vim xsync</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">=================================================</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if ((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=103; host&lt;105; host++)); do</span><br><span class="line">     echo --------- hadoop$host ----------</span><br><span class="line">        rsync -av $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br><span class="line">=================================================</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x xsync</span><br><span class="line">sudo cp xsync /bin</span><br><span class="line">sudo xsync /bin/xsync</span><br></pre></td></tr></table></figure>
<h2 id="八、配置免密登陆"><a href="#八、配置免密登陆" class="headerlink" title="八、配置免密登陆"></a>八、配置免密登陆</h2><p><img src="/2020/01/07/%E7%94%A8%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%9A%84%E9%9B%86%E7%BE%A4/01.png" alt></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.生成密钥对</span><br><span class="line">ssh-keygen -t rsa 三次回车</span><br><span class="line">2. 发送公钥到本机</span><br><span class="line">ssh-copy-id hadoop102 输入一次密码</span><br><span class="line">3. 分别ssh登陆一下所有虚拟机</span><br><span class="line">ssh hadoop103</span><br><span class="line">exit</span><br><span class="line">ssh hadoop104</span><br><span class="line">exit</span><br><span class="line">4. 把/home/atguigu/.ssh 文件夹发送到集群所有服务器</span><br><span class="line">xsync /home/atguigu/.ssh</span><br></pre></td></tr></table></figure>
<h2 id="九、在一台机器上安装Java和Hadoop，并配置环境变量，并分发到集群其他机器"><a href="#九、在一台机器上安装Java和Hadoop，并配置环境变量，并分发到集群其他机器" class="headerlink" title="九、在一台机器上安装Java和Hadoop，并配置环境变量，并分发到集群其他机器"></a>九、在一台机器上安装Java和Hadoop，并配置环境变量，并分发到集群其他机器</h2><h3 id="1-拷贝文件到-opt-software，两个tar包"><a href="#1-拷贝文件到-opt-software，两个tar包" class="headerlink" title="1.拷贝文件到/opt/software，两个tar包"></a>1.拷贝文件到/opt/software，两个tar包</h3><h3 id="2-tar-zxf-h”-tab”-C-opt-module"><a href="#2-tar-zxf-h”-tab”-C-opt-module" class="headerlink" title="2.tar -zxf h”+tab” -C /opt/module"></a>2.tar -zxf h”+tab” -C /opt/module</h3><h3 id="3-tar-zxf-j”-tab”-C-opt-module"><a href="#3-tar-zxf-j”-tab”-C-opt-module" class="headerlink" title="3.tar -zxf j”+tab” -C /opt/module"></a>3.tar -zxf j”+tab” -C /opt/module</h3><h3 id="4-sudo-vim-etc-profile"><a href="#4-sudo-vim-etc-profile" class="headerlink" title="4.sudo vim /etc/profile"></a>4.sudo vim /etc/profile</h3><p>在文件末尾添加</p>
  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>
<h3 id="5-source-etc-profile"><a href="#5-source-etc-profile" class="headerlink" title="5.source /etc/profile"></a>5.source /etc/profile</h3><h3 id="6-sudo-xsync-etc-profile"><a href="#6-sudo-xsync-etc-profile" class="headerlink" title="6.sudo xsync /etc/profile"></a>6.sudo xsync /etc/profile</h3><h3 id="7-在其他机器分别执行source-etc-profile"><a href="#7-在其他机器分别执行source-etc-profile" class="headerlink" title="7.在其他机器分别执行source /etc/profile"></a>7.在其他机器分别执行source /etc/profile</h3><p>所有配置文件都在$HADOOP_HOME/etc/hadoop</p>
<h2 id="十、首先配置hadoop-env-sh-yarn-env-sh-mapred-env-sh文件"><a href="#十、首先配置hadoop-env-sh-yarn-env-sh-mapred-env-sh文件" class="headerlink" title="十、首先配置hadoop-env.sh,yarn-env.sh,mapred-env.sh文件"></a>十、首先配置hadoop-env.sh,yarn-env.sh,mapred-env.sh文件</h2><p>配置JAVA_HOME,在每个文件第二行添加</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>

<h2 id="十一、配置Core-site-xml"><a href="#十一、配置Core-site-xml" class="headerlink" title="十一、配置Core-site.xml"></a>十一、配置Core-site.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定HDFS中NameNode的地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="十二、配置hdfs-site-xml"><a href="#十二、配置hdfs-site-xml" class="headerlink" title="十二、配置hdfs-site.xml"></a>十二、配置hdfs-site.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 数据的副本数量 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="十三、配置yarn-site-xml"><a href="#十三、配置yarn-site-xml" class="headerlink" title="十三、配置yarn-site.xml"></a>十三、配置yarn-site.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Reducer获取数据的方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定YARN的ResourceManager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="十四、配置mapred-site-xml"><a href="#十四、配置mapred-site-xml" class="headerlink" title="十四、配置mapred-site.xml"></a>十四、配置mapred-site.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>启动历史服务器：mr-jobhistory-daemon.sh start historyserver</p>
<h2 id="十五、配置Slaves"><a href="#十五、配置Slaves" class="headerlink" title="十五、配置Slaves"></a>十五、配置Slaves</h2><p>hadoop102<br>hadoop103<br>hadoop104</p>
<h2 id="十六、分发配置文件"><a href="#十六、分发配置文件" class="headerlink" title="十六、分发配置文件"></a>十六、分发配置文件</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop-2.7.2/etc</span><br></pre></td></tr></table></figure>

<h2 id="十七、格式化Namenode-在hadoop102"><a href="#十七、格式化Namenode-在hadoop102" class="headerlink" title="十七、格式化Namenode 在hadoop102"></a>十七、格式化Namenode 在hadoop102</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h2 id="十八、启动-停止hdfs"><a href="#十八、启动-停止hdfs" class="headerlink" title="十八、启动/停止hdfs"></a>十八、启动/停止hdfs</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-dfs.sh </span><br><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure>

<h2 id="十九、启动-停止yarn"><a href="#十九、启动-停止yarn" class="headerlink" title="十九、启动/停止yarn"></a>十九、启动/停止yarn</h2><p>在配置了Resourcemanager机器上执行<br>在Hadoop103上启动start-yarn.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start-yarn.sh </span><br><span class="line">stop-dfs.sh</span><br></pre></td></tr></table></figure>

<p>jps查看启动的集群各机器工作状态</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>个人学习笔记</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title>从Hadoop框架学习大数据生态</title>
    <url>/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/</url>
    <content><![CDATA[<p><strong>前言：</strong>现如今，大数据已经在各个领域运用广泛，具有Volume、Velocity、Variety、Value（即大量、高速、多样以及低价值密度）的特点，本文谨记录本人通过Hadoop框架学习大数据生态的过程以及如何在虚拟机上搭建一个简单的Hadoop运行环境，本文仅用于交流学习。图片以及资料来自于各大视频网站以及论坛，如出现表述错误的地方还请各位大佬指正~</p>
<h2 id="一、初识Hadoop"><a href="#一、初识Hadoop" class="headerlink" title="一、初识Hadoop"></a>一、初识Hadoop</h2><h3 id="1-什么是Hadoop"><a href="#1-什么是Hadoop" class="headerlink" title="1.什么是Hadoop"></a>1.什么是Hadoop</h3><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop01.png" alt></p>
<h3 id="2-Hadoop发展历史"><a href="#2-Hadoop发展历史" class="headerlink" title="2.Hadoop发展历史"></a>2.Hadoop发展历史</h3><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop02.png" alt></p>
<a id="more"></a>

<h3 id="3-Hadoop的优势（4高）"><a href="#3-Hadoop的优势（4高）" class="headerlink" title="3.Hadoop的优势（4高）"></a>3.Hadoop的优势（4高）</h3><p>（1）高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</p>
<p>（2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。</p>
<p>（3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</p>
<p>（4）高容错性：能够自动将失败的任务重新分配。</p>
<h3 id="4-Hadoop组成"><a href="#4-Hadoop组成" class="headerlink" title="4.Hadoop组成"></a>4.Hadoop组成</h3><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop03.png" alt></p>
<h4 id="（1-HDFS架构概述"><a href="#（1-HDFS架构概述" class="headerlink" title="（1)HDFS架构概述"></a>（1)HDFS架构概述</h4><ul>
<li><strong>NameNode（nn）</strong>：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。</li>
<li><strong>Data Node(dn)</strong>：在本地文件系统存储文件块数据，以及块数据的校验和。</li>
<li><strong>Secondary NameNode(2nn)</strong>：用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。</li>
</ul>
<h4 id="（2-YARN架构概述"><a href="#（2-YARN架构概述" class="headerlink" title="（2)YARN架构概述"></a>（2)YARN架构概述</h4><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop04.png" alt></p>
<h4 id="（3）MapReduce架构概述"><a href="#（3）MapReduce架构概述" class="headerlink" title="（3）MapReduce架构概述"></a>（3）MapReduce架构概述</h4><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop05.png" alt></p>
<h2 id="二、大数据技术生态体系"><a href="#二、大数据技术生态体系" class="headerlink" title="二、大数据技术生态体系"></a>二、大数据技术生态体系</h2><p><img src="/2020/01/02/%E4%BB%8EHadoop%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/Hadoop06.png" alt></p>
<p><strong>图中涉及的技术名词解释如下：</strong></p>
<ul>
<li><p><strong>Sqoop：</strong>Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库(MySql)间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p>
</li>
<li><p><strong>Flume：</strong>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。</p>
</li>
<li><p><strong>Kafka：</strong>Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性：</p>
<p>（1）通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。</p>
<p>（2）高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。</p>
<p>（3）支持通过Kafka服务器和消费机集群来分区消息。</p>
<p>（4）支持Hadoop并行数据加载。</p>
</li>
<li><p><strong>Storm：</strong>Storm用于“连续计算”，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。</p>
</li>
<li><p><strong>Spark：</strong>Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</p>
</li>
<li><p><strong>Oozie：</strong>Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。</p>
</li>
<li><p><strong>Hbase：</strong>HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p>
</li>
<li><p><strong>Hive：</strong>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p>
</li>
<li><p><strong>R语言</strong>：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。</p>
</li>
<li><p><strong>Mahout：</strong>Apache Mahout是个可扩展的机器学习和数据挖掘库。</p>
</li>
<li><p><strong>ZooKeeper：</strong>Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>
</li>
</ul>
<h2 id="三、Hadoop运行环境搭建"><a href="#三、Hadoop运行环境搭建" class="headerlink" title="三、Hadoop运行环境搭建"></a>三、Hadoop运行环境搭建</h2><h3 id="1-Hadoop下载"><a href="#1-Hadoop下载" class="headerlink" title="1.Hadoop下载"></a>1.Hadoop下载</h3><p>Hadoop下载地址：<a href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/" target="_blank" rel="noopener">https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/</a></p>
<p>由于我是运行在虚拟机（CentOs）上，故用XShell或SecureCRT工具将hadoop-2.7.2.tar.gz导入到opt目录下面的software文件夹下面。</p>
<h3 id="2-进入到Hadoop安装包路径下"><a href="#2-进入到Hadoop安装包路径下" class="headerlink" title="2.进入到Hadoop安装包路径下"></a>2.进入到Hadoop安装包路径下</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 ~]$ cd /opt/software/</span><br></pre></td></tr></table></figure>

<h3 id="3-解压安装文件到-opt-module下面"><a href="#3-解压安装文件到-opt-module下面" class="headerlink" title="3.解压安装文件到/opt/module下面"></a>3.解压安装文件到/opt/module下面</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 software]$ tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<h3 id="4-查看是否解压成功"><a href="#4-查看是否解压成功" class="headerlink" title="4.查看是否解压成功"></a>4.查看是否解压成功</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 software]$ ls /opt/module/</span><br><span class="line">hadoop-2.7.2</span><br></pre></td></tr></table></figure>

<h3 id="5-将Hadoop添加到环境变量"><a href="#5-将Hadoop添加到环境变量" class="headerlink" title="5.将Hadoop添加到环境变量"></a>5.将Hadoop添加到环境变量</h3><h4 id="1-获取Hadoop安装路径"><a href="#1-获取Hadoop安装路径" class="headerlink" title="(1)获取Hadoop安装路径"></a>(1)获取Hadoop安装路径</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 hadoop-2.7.2]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2</span><br></pre></td></tr></table></figure>

<h4 id="2-打开-etc-profile文件"><a href="#2-打开-etc-profile文件" class="headerlink" title="(2)打开/etc/profile文件"></a>(2)打开/etc/profile文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 hadoop-2.7.2]$ sudo vim /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="3-在profile文件末尾添加JDK路径"><a href="#3-在profile文件末尾添加JDK路径" class="headerlink" title="(3)在profile文件末尾添加JDK路径"></a>(3)在profile文件末尾添加JDK路径</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#HADOOP_HOME</span></span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<h4 id="4-让修改后的文件生效"><a href="#4-让修改后的文件生效" class="headerlink" title="(4)让修改后的文件生效"></a>(4)让修改后的文件生效</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@ hadoop101 hadoop-2.7.2]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="5-测试是否安装成功"><a href="#5-测试是否安装成功" class="headerlink" title="(5)测试是否安装成功"></a>(5)测试是否安装成功</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hep@hadoop101 hadoop-2.7.2]$ hadoop version</span><br><span class="line">Hadoop 2.7.2</span><br></pre></td></tr></table></figure>

<h3 id="6-了解Hadoop的目录结构"><a href="#6-了解Hadoop的目录结构" class="headerlink" title="6.了解Hadoop的目录结构"></a>6.了解Hadoop的目录结构</h3><ul>
<li><p><strong>bin目录：</strong>存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本</p>
</li>
<li><p><strong>etc目录：</strong>Hadoop的配置文件目录，存放Hadoop的配置文件</p>
</li>
<li><p><strong>lib目录：</strong>存放Hadoop的本地库（对数据进行压缩解压缩功能）</p>
</li>
<li><p><strong>sbin目录：</strong>存放启动或停止Hadoop相关服务的脚本</p>
</li>
<li><p><strong>share目录：</strong>存放Hadoop的依赖jar包、文档、和官方案例</p>
</li>
</ul>
<h2 id="四、结语"><a href="#四、结语" class="headerlink" title="四、结语"></a>四、结语</h2><p>至此，已经粗略通过对Hadoop框架的学习大致了解了大数据的生态，Hadoop的环境也已经搭好。接下来将对hdfs、yarn以及mapreduce进行深入的了解。</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>个人学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown笔记</title>
    <url>/2019/12/31/Markdown%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><strong>前言：</strong>Markdown一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。本文主要记录Markdown的一些基本语法。</p>
<h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h2><h3 id="1-语法格式"><a href="#1-语法格式" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="section">#(空格)一级标题</span></span><br><span class="line"><span class="section">##(空格)二级标题</span></span><br><span class="line"><span class="section">###(空格)三级标题</span></span><br><span class="line"><span class="section">####(空格)四级标题</span></span><br><span class="line"><span class="section">#####(空格)五级标题</span></span><br><span class="line"><span class="section">######(空格)六级标题</span></span><br></pre></td></tr></table></figure>

<h3 id="2-效果图"><a href="#2-效果图" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><img src="/2019/12/31/Markdown%E7%AC%94%E8%AE%B0/markdown_01.png" style="zoom:50%;">

<a id="more"></a>

<h2 id="2-字体样式"><a href="#2-字体样式" class="headerlink" title="2.字体样式"></a>2.字体样式</h2><h3 id="1-语法格式-1"><a href="#1-语法格式-1" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="strong">**加粗的语言**</span></span><br><span class="line">~~被删除的文字~~</span><br><span class="line"><span class="emphasis">*斜体内容*</span></span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-1"><a href="#2-效果图-1" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><p><strong>加粗的语言</strong><br><del>被删除的文字</del><br><em>斜体内容*</em></p>
<h2 id="3-引用"><a href="#3-引用" class="headerlink" title="3.引用"></a>3.引用</h2><h3 id="1-语法格式-2"><a href="#1-语法格式-2" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">&gt;一级引用</span><br><span class="line">&gt;&gt;二级引用</span><br><span class="line">&gt;&gt;&gt;三级引用</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-2"><a href="#2-效果图-2" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><blockquote>
<p>一级引用</p>
<blockquote>
<p>二级引用</p>
<blockquote>
<p>三级引用</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="4-分割线"><a href="#4-分割线" class="headerlink" title="4.分割线"></a>4.分割线</h2><h3 id="1-语法格式-3"><a href="#1-语法格式-3" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">---分割线1</span><br><span class="line"><span class="emphasis">***</span>分割线2</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-3"><a href="#2-效果图-3" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><hr>
<hr>
<h2 id="5-图片插入"><a href="#5-图片插入" class="headerlink" title="5.图片插入"></a>5.图片插入</h2><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">!(我的照片)[图片路径]</span><br></pre></td></tr></table></figure>

<p><img src="/2019/12/31/Markdown%E7%AC%94%E8%AE%B0/markdown_02.png" alt></p>
<h2 id="6-超链接"><a href="#6-超链接" class="headerlink" title="6.超链接"></a>6.超链接</h2><h3 id="1-语法格式-4"><a href="#1-语法格式-4" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">[链接名称]（链接地址）</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-4"><a href="#2-效果图-4" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><p><a href="https://www.baidu.com" target="_blank" rel="noopener">百度一下</a></p>
<h2 id="7-代码块"><a href="#7-代码块" class="headerlink" title="7.代码块"></a>7.代码块</h2><h3 id="1-语法格式-5"><a href="#1-语法格式-5" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">​<span class="code">```</span>代码语言（java）</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-5"><a href="#2-效果图-5" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.math.BigDecimal;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestBody;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.api.EventApi;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.service.EventService;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.common.base.util.BeanUtil;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.vo.EventVo;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.vo.ResponseVo;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.se.school.operation.center.vo.SelectValueVo;</span><br><span class="line"><span class="keyword">import</span> com.cetccity.common.base.vo.PageVo;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventController</span> <span class="keyword">implements</span> <span class="title">EventApi</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Autowired</span></span><br><span class="line">	EventService eventService;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> PageVo&lt;EventVo&gt; <span class="title">queryEvent</span><span class="params">(@RequestBody(required=<span class="keyword">false</span>)</span> EventVo vo)</span>&#123;</span><br><span class="line">		List&lt;EventVo&gt; list = eventService.queryEvent(vo);</span><br><span class="line">		<span class="keyword">return</span> BeanUtil.page(list);	</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">addEvent</span><span class="params">(@RequestBody EventVo vo)</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> eventService.addEvent(vo);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> EventVo <span class="title">findEventById</span><span class="params">(@RequestParam(<span class="string">"id"</span>)</span> Long id)</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> eventService.findEventById(id);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">updateEventById</span><span class="params">(@RequestBody EventVo vo)</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> eventService.updateEventById(vo);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">delEventById</span><span class="params">(@RequestParam(<span class="string">"id"</span>)</span> Long id)</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> eventService.delEventById(id);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<h2 id="8-列表"><a href="#8-列表" class="headerlink" title="8.列表"></a>8.列表</h2><h3 id="1-语法格式-6"><a href="#1-语法格式-6" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">-(空格)无序列表目录1</span><br><span class="line"> -(空格)无序列表目录2</span><br><span class="line">  -(空格)无序列表目录3</span><br><span class="line"></span><br><span class="line">1.有序列表目录1</span><br><span class="line">2.有序列表目录2</span><br><span class="line">3.有序列表目录3</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-6"><a href="#2-效果图-6" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><ul>
<li>无序列表目录1<ul>
<li>无序列表目录2<ul>
<li>无序列表目录3</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>1.有序列表目录1<br>2.有序列表目录2<br>3.有序列表目录3</p>
<h2 id="9-表格"><a href="#9-表格" class="headerlink" title="9.表格"></a>9.表格</h2><h3 id="1-语法格式-7"><a href="#1-语法格式-7" class="headerlink" title="(1)语法格式"></a>(1)语法格式</h3><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">| 姓名 | 学号 | 成绩 |</span><br><span class="line">| :--: | :--: | :--: |</span><br><span class="line">|      |      |      |</span><br><span class="line">|      |      |      |</span><br><span class="line">|      |      |      |</span><br></pre></td></tr></table></figure>

<h3 id="2-效果图-7"><a href="#2-效果图-7" class="headerlink" title="(2)效果图"></a>(2)效果图</h3><table>
<thead>
<tr>
<th align="center">姓名</th>
<th align="center">学号</th>
<th align="center">成绩</th>
</tr>
</thead>
<tbody><tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>个人学习笔记</tag>
        <tag>Markdown</tag>
        <tag>基本语法</tag>
      </tags>
  </entry>
</search>
