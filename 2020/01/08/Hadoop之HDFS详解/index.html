<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Hadoop之HDFS详解 |  Hepn Blog
  </title>
  
  <link rel="shortcut icon" href="/hp.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<meta name="generator" content="Hexo 4.2.0"></head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-Hadoop之HDFS详解" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Hadoop之HDFS详解
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/" class="article-date">
  <time datetime="2020-01-08T05:49:01.000Z" itemprop="datePublished">2020-01-08</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
  </div>

    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="第1章-HDFS概述"><a href="#第1章-HDFS概述" class="headerlink" title="第1章 HDFS概述"></a>第1章 HDFS概述</h2><h3 id="1-1-HDFS产生背景"><a href="#1-1-HDFS产生背景" class="headerlink" title="1.1 HDFS产生背景"></a>1.1 HDFS产生背景</h3><p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。</p>
<h3 id="1-2HDFS的定义"><a href="#1-2HDFS的定义" class="headerlink" title="1.2HDFS的定义"></a>1.2HDFS的定义</h3><p>HDFS（Hadoop Distributed File System），它是一个<strong>文件系统</strong>，用于存储文件，通过目录树来定位文件；<strong>其次，它是分布式的</strong>，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<br><strong>HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。</strong>适合用来做数据分析，并不适合用来做网盘应用。</p>
<h3 id="1-3HDFS优缺点"><a href="#1-3HDFS优缺点" class="headerlink" title="1.3HDFS优缺点"></a>1.3HDFS优缺点</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/01.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/E:%5Cblog%5Csource_posts%5CHadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3%5C02.png" alt></p>
<h3 id="1-4-HDFS组成架构"><a href="#1-4-HDFS组成架构" class="headerlink" title="1.4 HDFS组成架构"></a>1.4 HDFS组成架构</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/03.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/04.png" alt></p>
<a id="more"></a>

<h3 id="1-5-HDFS文件块大小"><a href="#1-5-HDFS文件块大小" class="headerlink" title="1.5 HDFS文件块大小"></a>1.5 HDFS文件块大小</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/05.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/06.png" alt></p>
<h2 id="第2章-HDFS的Shell操作"><a href="#第2章-HDFS的Shell操作" class="headerlink" title="第2章 HDFS的Shell操作"></a>第2章 HDFS的Shell操作</h2><h3 id="2-1基本语法"><a href="#2-1基本语法" class="headerlink" title="2.1基本语法"></a>2.1基本语法</h3><p><strong>bin/hadoop fs</strong> 具体命令  或者 <strong>bin/hdfs dfs</strong> 具体命令</p>
<h3 id="2-2常用命令实操"><a href="#2-2常用命令实操" class="headerlink" title="2.2常用命令实操"></a>2.2常用命令实操</h3><h4 id="（0）启动Hadoop集群"><a href="#（0）启动Hadoop集群" class="headerlink" title="（0）启动Hadoop集群"></a>（0）启动Hadoop集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[hep@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="（1）-help：输出这个命令参数"><a href="#（1）-help：输出这个命令参数" class="headerlink" title="（1）-help：输出这个命令参数"></a>（1）-help：输出这个命令参数</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm</span><br></pre></td></tr></table></figure>

<h4 id="（2）-ls-显示目录信息"><a href="#（2）-ls-显示目录信息" class="headerlink" title="（2）-ls: 显示目录信息"></a>（2）-ls: 显示目录信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /</span><br></pre></td></tr></table></figure>

<h4 id="（3）-mkdir：在HDFS上创建目录"><a href="#（3）-mkdir：在HDFS上创建目录" class="headerlink" title="（3）-mkdir：在HDFS上创建目录"></a>（3）-mkdir：在HDFS上创建目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<h4 id="（4）-moveFromLocal：从本地剪切粘贴到HDFS"><a href="#（4）-moveFromLocal：从本地剪切粘贴到HDFS" class="headerlink" title="（4）-moveFromLocal：从本地剪切粘贴到HDFS"></a>（4）-moveFromLocal：从本地剪切粘贴到HDFS</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch kongming.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<h4 id="（5）-appendToFile：追加一个文件到已经存在的文件末尾"><a href="#（5）-appendToFile：追加一个文件到已经存在的文件末尾" class="headerlink" title="（5）-appendToFile：追加一个文件到已经存在的文件末尾"></a>（5）-appendToFile：追加一个文件到已经存在的文件末尾</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch liubei.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ vi liubei.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（6）-cat：显示文件内容"><a href="#（6）-cat：显示文件内容" class="headerlink" title="（6）-cat：显示文件内容"></a>（6）-cat：显示文件内容</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（7）-chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"><a href="#（7）-chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限" class="headerlink" title="（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"></a>（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -chown hep:hep  /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"><a href="#（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去" class="headerlink" title="（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"></a>（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /</span><br></pre></td></tr></table></figure>

<h4 id="（9）-copyToLocal：从HDFS拷贝到本地"><a href="#（9）-copyToLocal：从HDFS拷贝到本地" class="headerlink" title="（9）-copyToLocal：从HDFS拷贝到本地"></a>（9）-copyToLocal：从HDFS拷贝到本地</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>

<h4 id="（10）-cp-：从HDFS的一个路径拷贝到HDFS的另一个路径"><a href="#（10）-cp-：从HDFS的一个路径拷贝到HDFS的另一个路径" class="headerlink" title="（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径"></a>（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt</span><br></pre></td></tr></table></figure>

<h4 id="（11）-mv：在HDFS目录中移动文件"><a href="#（11）-mv：在HDFS目录中移动文件" class="headerlink" title="（11）-mv：在HDFS目录中移动文件"></a>（11）-mv：在HDFS目录中移动文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/</span><br></pre></td></tr></table></figure>

<h4 id="（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地"><a href="#（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地" class="headerlink" title="（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地"></a>（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>

<h4 id="（13）-getmerge：合并下载多个文件"><a href="#（13）-getmerge：合并下载多个文件" class="headerlink" title="（13）-getmerge：合并下载多个文件"></a>（13）-getmerge：合并下载多个文件</h4><p>比如HDFS的目录 /user/hep/test下有多个文件:log.1, log.2,log.3,…</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/hep/test/* ./zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<h4 id="（14）-put：等同于copyFromLocal"><a href="#（14）-put：等同于copyFromLocal" class="headerlink" title="（14）-put：等同于copyFromLocal"></a>（14）-put：等同于copyFromLocal</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/hep/test/</span><br></pre></td></tr></table></figure>

<h4 id="（15）-tail：显示一个文件的末尾"><a href="#（15）-tail：显示一个文件的末尾" class="headerlink" title="（15）-tail：显示一个文件的末尾"></a>（15）-tail：显示一个文件的末尾</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（16）-rm：删除文件或文件夹"><a href="#（16）-rm：删除文件或文件夹" class="headerlink" title="（16）-rm：删除文件或文件夹"></a>（16）-rm：删除文件或文件夹</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/hep/test/jinlian2.txt</span><br></pre></td></tr></table></figure>

<h4 id="（17）-rmdir：删除空目录"><a href="#（17）-rmdir：删除空目录" class="headerlink" title="（17）-rmdir：删除空目录"></a>（17）-rmdir：删除空目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test</span><br></pre></td></tr></table></figure>

<h4 id="（18）-du统计文件夹的大小信息"><a href="#（18）-du统计文件夹的大小信息" class="headerlink" title="（18）-du统计文件夹的大小信息"></a>（18）-du统计文件夹的大小信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/hep/test</span><br></pre></td></tr></table></figure>

<p><strong>结果：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2.7 K /user/hep/test</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/hep/test</span><br></pre></td></tr></table></figure>

<p><strong>结果：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.3 K &#x2F;user&#x2F;hep&#x2F;test&#x2F;README.txt</span><br><span class="line">15   &#x2F;user&#x2F;hep&#x2F;test&#x2F;jinlian.txt</span><br><span class="line">1.4 K &#x2F;user&#x2F;hep&#x2F;test&#x2F;zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<h4 id="（19）-setrep：设置HDFS中文件的副本数量"><a href="#（19）-setrep：设置HDFS中文件的副本数量" class="headerlink" title="（19）-setrep：设置HDFS中文件的副本数量"></a>（19）-setrep：设置HDFS中文件的副本数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 &#x2F;sanguo&#x2F;shuguo&#x2F;kongming.txt</span><br></pre></td></tr></table></figure>

<p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
<h2 id="第3章-HDFS客户端操作"><a href="#第3章-HDFS客户端操作" class="headerlink" title="第3章 HDFS客户端操作"></a>第3章 HDFS客户端操作</h2><h3 id="3-1-HDFS客户端环境准备"><a href="#3-1-HDFS客户端环境准备" class="headerlink" title="3.1 HDFS客户端环境准备"></a>3.1 HDFS客户端环境准备</h3><h4 id="1．拷贝对应的编译后的hadoop-jar包到非中文路径"><a href="#1．拷贝对应的编译后的hadoop-jar包到非中文路径" class="headerlink" title="1．拷贝对应的编译后的hadoop jar包到非中文路径"></a>1．拷贝对应的编译后的hadoop jar包到非中文路径</h4><h4 id="2．配置HADOOP-HOME环境变量"><a href="#2．配置HADOOP-HOME环境变量" class="headerlink" title="2．配置HADOOP_HOME环境变量"></a>2．配置HADOOP_HOME环境变量</h4><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/07.png" alt></p>
<h4 id="3-配置Path环境变量"><a href="#3-配置Path环境变量" class="headerlink" title="3.配置Path环境变量"></a>3.配置Path环境变量</h4><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/08.png" alt></p>
<h4 id="4-执行winutils判断是否配置成功"><a href="#4-执行winutils判断是否配置成功" class="headerlink" title="4.执行winutils判断是否配置成功"></a>4.执行winutils判断是否配置成功</h4><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/09.png" alt></p>
<h4 id="5-创建一个Maven工程HdfsClient"><a href="#5-创建一个Maven工程HdfsClient" class="headerlink" title="5.创建一个Maven工程HdfsClient"></a>5.创建一个Maven工程HdfsClient</h4><h4 id="6-导入相应的依赖坐标-日志添加"><a href="#6-导入相应的依赖坐标-日志添加" class="headerlink" title="6.导入相应的依赖坐标+日志添加"></a>6.导入相应的依赖坐标+日志添加</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：如果Eclipse/Idea打印不出日志，在控制台上只显示</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">1.log4j</span>:<span class="string">WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). </span></span><br><span class="line"><span class="meta">2.log4j</span>:<span class="string">WARN Please initialize the log4j system properly. </span></span><br><span class="line"><span class="meta">3.log4j</span>:<span class="string">WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span></span><br></pre></td></tr></table></figure>

<p><strong>需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<h4 id="7-创建包名：com-hep-hdfs"><a href="#7-创建包名：com-hep-hdfs" class="headerlink" title="7.创建包名：com.hep.hdfs"></a>7.创建包名：com.hep.hdfs</h4><h4 id="8-创建HdfsClient类"><a href="#8-创建HdfsClient类" class="headerlink" title="8.创建HdfsClient类"></a>8.创建HdfsClient类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span>&#123;  </span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line">   <span class="comment">// 1 获取文件系统</span></span><br><span class="line">   Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">   <span class="comment">// 配置在集群上运行</span></span><br><span class="line">   <span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span></span><br><span class="line">   <span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line">   FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line">   <span class="comment">// 2 创建目录</span></span><br><span class="line">   fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/new"</span>));</span><br><span class="line">   <span class="comment">// 3 关闭资源</span></span><br><span class="line">   fs.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2HDFS的API操作"><a href="#3-2HDFS的API操作" class="headerlink" title="3.2HDFS的API操作"></a>3.2HDFS的API操作</h3><h4 id="1-HDFS文件下载"><a href="#1-HDFS文件下载" class="headerlink" title="1.HDFS文件下载"></a>1.HDFS文件下载</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//获取一个HDFS的抽象封装对象</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fileSystem = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line"><span class="comment">//用这个对象操作文件系统</span></span><br><span class="line"> fileSystem.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/test"</span>), <span class="keyword">new</span> Path(<span class="string">"e:\\"</span>));</span><br><span class="line"><span class="comment">//关闭文件系统</span></span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-HDFS文件名更改"><a href="#2-HDFS文件名更改" class="headerlink" title="2.HDFS文件名更改"></a>2.HDFS文件名更改</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">FileSystem fileSystem = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"hep"</span>);</span><br><span class="line">fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/test"</span>), <span class="keyword">new</span> Path(<span class="string">"/test2"</span>));</span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-HDFS文件删除"><a href="#3-HDFS文件删除" class="headerlink" title="3.HDFS文件删除"></a>3.HDFS文件删除</h4><p>此处因为每一次对文件系统进行操作前都需要获取一个HDFS的抽象封装对象，以及在使用完之后关闭文件系统。于是我在测试前写了个@Before和@After，便于后续的测试。之后的API测试均由生成的fs替代。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> FileSystem fs;</span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">before</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"hep"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Before!!!!!"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">after</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    s.close();</span><br><span class="line">    System.out.println(<span class="string">"After!!!!!!!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> delete = fs.delete(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">if</span> (delete) &#123;</span><br><span class="line">        System.out.println(<span class="string">"删除成功"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">"删除失败"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-HDFS文件的追加"><a href="#4-HDFS文件的追加" class="headerlink" title="4.HDFS文件的追加"></a>4.HDFS文件的追加</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    FSDataOutputStream append = fs.append(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>),<span class="number">1024</span>);</span><br><span class="line">    FileInputStream open = <span class="keyword">new</span> FileInputStream(<span class="string">"E://test/1.txt"</span>);</span><br><span class="line">    IOUtils.copyBytes(open,append,<span class="number">1024</span>,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-HDFS文件详情查看以及文件和文件夹判断"><a href="#5-HDFS文件详情查看以及文件和文件夹判断" class="headerlink" title="5.HDFS文件详情查看以及文件和文件夹判断"></a>5.HDFS文件详情查看以及文件和文件夹判断</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    <span class="keyword">for</span>(FileStatus fileStatus : fileStatuses)&#123;</span><br><span class="line">        <span class="keyword">if</span>(fileStatus.isFile())&#123;</span><br><span class="line">            System.out.println(<span class="string">"以下信息是一个文件的信息："</span>);</span><br><span class="line">            System.out.println(fileStatus.getPath());</span><br><span class="line">            System.out.println(fileStatus.getLen());</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"这是一个文件夹"</span>);</span><br><span class="line">            System.out.println(fileStatus.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="6-列出路径下所有的文件块信息"><a href="#6-列出路径下所有的文件块信息" class="headerlink" title="6.列出路径下所有的文件块信息"></a>6.列出路径下所有的文件块信息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>),<span class="keyword">true</span>);<span class="comment">//此处的true是指递归深入文件夹中的文件夹(只能查文件，因为文件夹没有block)</span></span><br><span class="line">    <span class="keyword">while</span>(files.hasNext()) &#123;</span><br><span class="line">        LocatedFileStatus file = files.next();</span><br><span class="line">        System.out.println(<span class="string">"============================"</span>);</span><br><span class="line">        System.out.println(file.getPath());</span><br><span class="line">        System.out.println(<span class="string">"块信息："</span>);</span><br><span class="line">        BlockLocation[] blockLocations = file.getBlockLocations();</span><br><span class="line">        <span class="keyword">for</span>(BlockLocation blockLocation : blockLocations)&#123;</span><br><span class="line">            String[] hosts = blockLocation.getHosts();</span><br><span class="line">            System.out.print(<span class="string">"块在"</span>);</span><br><span class="line">            <span class="keyword">for</span> (String host : hosts)&#123;</span><br><span class="line">                System.out.println(host+<span class="string">" "</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="7-HDFS文件上传"><a href="#7-HDFS文件上传" class="headerlink" title="7.HDFS文件上传"></a>7.HDFS文件上传</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">()</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    configuration.setInt(<span class="string">"dfs.replication"</span>,<span class="number">1</span>);<span class="comment">//设置配置文件</span></span><br><span class="line">    fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line">    fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"e:/test/1.txt"</span>),<span class="keyword">new</span> Path(<span class="string">"/2.txt"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此处的configuration有自定的配置，若api里有配置文件的设置，api的优先级更高，若没有则看配置文件中的配置。</p>
<h2 id="第4章-HDFS的数据流"><a href="#第4章-HDFS的数据流" class="headerlink" title="第4章 HDFS的数据流"></a>第4章 HDFS的数据流</h2><h3 id="4-1-HDFS写数据流程"><a href="#4-1-HDFS写数据流程" class="headerlink" title="4.1 HDFS写数据流程"></a>4.1 HDFS写数据流程</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/10.png" alt></p>
<ol>
<li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</li>
<li>NameNode返回是否可以上传。</li>
<li>客户端请求第一个 Block上传到哪几个DataNode服务器上。</li>
<li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</li>
<li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</li>
<li>dn1、dn2、dn3逐级应答客户端。</li>
<li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
</ol>
<h3 id="4-2-HDFS读数据流程"><a href="#4-2-HDFS读数据流程" class="headerlink" title="4.2 HDFS读数据流程"></a>4.2 HDFS读数据流程</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/11.png" alt></p>
<ol>
<li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<h3 id="4-3网络拓扑"><a href="#4-3网络拓扑" class="headerlink" title="4.3网络拓扑"></a>4.3网络拓扑</h3><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。</p>
<p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/12.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/13.png" alt></p>
<h3 id="4-4机架感知（副本存储节点选择）"><a href="#4-4机架感知（副本存储节点选择）" class="headerlink" title="4.4机架感知（副本存储节点选择）"></a>4.4机架感知（副本存储节点选择）</h3><blockquote>
<p>For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack.</p>
</blockquote>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/14.png" alt></p>
<h2 id="第5章-NameNode和SecondaryNameNode"><a href="#第5章-NameNode和SecondaryNameNode" class="headerlink" title="第5章 NameNode和SecondaryNameNode"></a>第5章 NameNode和SecondaryNameNode</h2><h3 id="5-1-NN和2NN工作机制"><a href="#5-1-NN和2NN工作机制" class="headerlink" title="5.1 NN和2NN工作机制"></a>5.1 NN和2NN工作机制</h3><blockquote>
<p><strong>思考：NameNode中的元数据是存储在哪里的？</strong></p>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。<strong>因此产生在磁盘中备份元数据的FsImage。</strong></p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失<strong>。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。</strong>这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。<strong>因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</strong></p>
</blockquote>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/15.png" alt></p>
<h4 id="1-第一阶段：NameNode启动"><a href="#1-第一阶段：NameNode启动" class="headerlink" title="1.第一阶段：NameNode启动"></a>1.第一阶段：NameNode启动</h4><p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>（2）客户端对元数据进行增删改的请求。<br>（3）NameNode记录操作日志，更新滚动日志。<br>（4）NameNode在内存中对元数据进行增删改。</p>
<h4 id="2-第二阶段：Secondary-NameNode工作"><a href="#2-第二阶段：Secondary-NameNode工作" class="headerlink" title="2.第二阶段：Secondary NameNode工作"></a>2.第二阶段：Secondary NameNode工作</h4><p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。<br>（2）Secondary NameNode请求执行CheckPoint。<br>（3）NameNode滚动正在写的Edits日志。<br>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br>（6）生成新的镜像文件fsimage.chkpoint。<br>（7）拷贝fsimage.chkpoint到NameNode。<br>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<hr>
<blockquote>
<p><strong>NN和2NN工作机制详解：</strong></p>
<p><strong>Fsimage：</strong>NameNode内存中元数据序列化后形成的文件。</p>
<p><strong>Edits：</strong>记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</p>
<p>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（<strong>所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage</strong>）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</p>
<p>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（<strong>触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了</strong>）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p>
</blockquote>
<h3 id="5-2-Fsimage和Edits解析"><a href="#5-2-Fsimage和Edits解析" class="headerlink" title="5.2 Fsimage和Edits解析"></a>5.2 Fsimage和Edits解析</h3><h4 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h4><p>​    NameNode被格式化之后，将在<strong>/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录</strong>中产生如下文件：<br><strong>（1）Fsimage文件：</strong>HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。<br><strong>（2）Edits文件：</strong>存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。<br><strong>（3）seen_txid文件：</strong>保存的是一个数字，就是最后一个edits_的数字<br>（4）每次NameNode<strong>启动的时候</strong>都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。</p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/16.png" alt></p>
<h4 id="2-oiv查看Fsimage文件"><a href="#2-oiv查看Fsimage文件" class="headerlink" title="2.oiv查看Fsimage文件"></a>2.oiv查看Fsimage文件</h4><h5 id="（1）查看oiv和oev命令"><a href="#（1）查看oiv和oev命令" class="headerlink" title="（1）查看oiv和oev命令"></a>（1）查看oiv和oev命令</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 current]$ hdfs</span><br><span class="line"></span><br><span class="line">**oiv**  apply the offline fsimage viewer to an fsimage</span><br><span class="line"></span><br><span class="line">**oev**  apply the offline edits viewer to an edits file</span><br></pre></td></tr></table></figure>

<h5 id="（2）基本语法"><a href="#（2）基本语法" class="headerlink" title="（2）基本语法"></a>（2）基本语法</h5><p><strong>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</strong></p>
<h5 id="（3）案例实操"><a href="#（3）案例实操" class="headerlink" title="（3）案例实操"></a>（3）案例实操</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 current]$  hdfs oiv -p XML -i fsimage_0000000000000000991 -o /opt/module/hadoop-2.7.2/fsimage.xml</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 current]$  hdfs oev -p XML -i edits_0000000000000000888-0000000000000000940 -o /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure>

<p>将显示的xml文件内容打开,部分显示结果如下。</p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/17.png" alt></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/18.png" alt></p>
<h3 id="5-3-CheckPoint时间设置"><a href="#5-3-CheckPoint时间设置" class="headerlink" title="5.3 CheckPoint时间设置"></a>5.3 CheckPoint时间设置</h3><h4 id="1-通常情况下，SecondaryNameNode每隔一小时执行一次。"><a href="#1-通常情况下，SecondaryNameNode每隔一小时执行一次。" class="headerlink" title="1.通常情况下，SecondaryNameNode每隔一小时执行一次。"></a>1.通常情况下，SecondaryNameNode每隔一小时执行一次。</h4><p>​    <strong>[hdfs-default.xml]</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="2-自定义设置"><a href="#2-自定义设置" class="headerlink" title="2.自定义设置"></a>2.自定义设置</h4><p>一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="5-4-NameNode故障处理"><a href="#5-4-NameNode故障处理" class="headerlink" title="5.4 NameNode故障处理"></a>5.4 NameNode故障处理</h3><p>NameNode故障后，可以采用如下两种方法恢复数据。</p>
<p><strong>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</strong></p>
<ol>
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 dfs]$ scp -r hep@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>重新启动NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<p><strong>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</strong></p>
<ol>
<li>修改hdfs-site.xml中的</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 dfs]$ scp -r hep@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 namesecondary]$ rm -rf in_use.lock</span><br><span class="line"> </span><br><span class="line">[hep@hadoop102 dfs]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/data/tmp/dfs</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 dfs]$ ls</span><br><span class="line">data name namesecondary</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>导入检查点数据（等待一会ctrl+c结束掉）</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>启动NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure>

<h3 id="5-5-集群安全模式"><a href="#5-5-集群安全模式" class="headerlink" title="5.5 集群安全模式"></a>5.5 集群安全模式</h3><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h4><p><strong>NameNode启动:</strong>NameNode启动时，首先将镜像文件（Fsimage）载入内存，并执行编辑日志（Edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个新的Fsimage文件和一个空的编辑日志。此时，NameNode开始监听DataNode请求。这个过程期间，NameNode一直运行在安全模式，即NameNode的文件系统对于客户端来说是只读的。</p>
<p><strong>DateNode启动:</strong>系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。</p>
<p><strong>安全模式退出判断：</strong>如果满足“最小副本条件”，NameNode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。</p>
<h4 id="2-基本语法"><a href="#2-基本语法" class="headerlink" title="2.基本语法"></a>2.基本语法</h4><p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p>
<p>（1）bin/hdfs dfsadmin -safemode get    （功能描述：查看安全模式状态）</p>
<p>（2）bin/hdfs dfsadmin -safemode enter  （功能描述：进入安全模式状态）</p>
<p>（3）bin/hdfs dfsadmin -safemode leave   （功能描述：离开安全模式状态）</p>
<p>（4）bin/hdfs dfsadmin -safemode wait   （功能描述：等待安全模式状态）</p>
<h4 id="3-案例-模拟等待安全模式"><a href="#3-案例-模拟等待安全模式" class="headerlink" title="3.案例-模拟等待安全模式"></a>3.案例-模拟等待安全模式</h4><p>（1）查看当前模式</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -safemode get</span><br></pre></td></tr></table></figure>
<p>  Safe mode is OFF<br>（2）先进入安全模式</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode enter</span><br></pre></td></tr></table></figure>
<p> （3）创建并执行下面的脚本<br>在/opt/module/hadoop-2.7.2路径上，编辑一个脚本safemode.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch safemode.sh</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ vim safemode.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">hdfs dfsadmin -safemode wait</span><br><span class="line">hdfs dfs -put /opt/module/hadoop-2.7.2/README.txt /</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ chmod 777 safemode.sh</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ ./safemode.sh</span><br></pre></td></tr></table></figure>
<p>（4）再打开一个窗口，执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure>
<p>（5）再观察上一个窗口<br> Safe mode is OFF    HDFS集群上已经有上传的数据了。</p>
<h2 id="第6章-DataNode"><a href="#第6章-DataNode" class="headerlink" title="第6章 DataNode"></a>第6章 DataNode</h2><h3 id="6-1-DataNode工作机制"><a href="#6-1-DataNode工作机制" class="headerlink" title="6.1 DataNode工作机制"></a>6.1 DataNode工作机制</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/19.png" alt></p>
<ol>
<li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</li>
<li>DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</li>
<li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。</li>
<li>集群运行中可以安全加入和退出一些机器。</li>
</ol>
<h3 id="6-2-掉线时限参数设置"><a href="#6-2-掉线时限参数设置" class="headerlink" title="6.2 掉线时限参数设置"></a>6.2 掉线时限参数设置</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/20.png" alt></p>
<p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为<strong>毫秒</strong>，dfs.heartbeat.interval的单位为<strong>秒</strong>。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="6-3-数据完整性"><a href="#6-3-数据完整性" class="headerlink" title="6.3 数据完整性"></a>6.3 数据完整性</h3><p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/21.png" alt></p>
<p><strong>DataNode节点保证数据完整性的方法:</strong></p>
<ol>
<li>当DataNode读取Block的时候，它会计算CheckSum。</li>
<li>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</li>
<li>Client读取其他DataNode上的Block。</li>
<li>DataNode在其文件创建后周期验证CheckSum</li>
</ol>
<h3 id="6-4服役新数据节点"><a href="#6-4服役新数据节点" class="headerlink" title="6.4服役新数据节点"></a>6.4服役新数据节点</h3><p>随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点。</p>
<h4 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1.环境准备"></a>1.环境准备</h4><p><strong>（1）在hadoop104主机上再克隆一台hadoop105主机</strong></p>
<p><strong>（2）修改IP地址和主机名称</strong></p>
<p><strong>（3）删除原来HDFS文件系统留存的文件（/opt/module/hadoop-2.7.2/data和log）</strong></p>
<p><strong>（4）source一下配置文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop105 hadoop-2.7.2]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="2-服役新节点具体步骤"><a href="#2-服役新节点具体步骤" class="headerlink" title="2.服役新节点具体步骤"></a>2.服役新节点具体步骤</h4><p><strong>（1）直接启动DataNode，即可关联到集群</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start datanode</span><br><span class="line"></span><br><span class="line">[hep@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh start nodemanage</span><br></pre></td></tr></table></figure>

<p><strong>（2）在hadoop105上上传文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop105 hadoop-2.7.2]$ hadoop fs -put /opt/module/hadoop-2.7.2/LICENSE.txt /</span><br></pre></td></tr></table></figure>

<p><strong>（3）如果数据不均衡，可以用命令实现集群的再平衡</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 sbin]$ ./start-balancer.sh</span><br></pre></td></tr></table></figure>

<h3 id="6-5-退役旧数据节点"><a href="#6-5-退役旧数据节点" class="headerlink" title="6.5 退役旧数据节点"></a>6.5 退役旧数据节点</h3><h4 id="1-添加白名单"><a href="#1-添加白名单" class="headerlink" title="1. 添加白名单"></a>1. 添加白名单</h4><p>​    添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。</p>
<p>配置白名单的具体步骤如下：</p>
<p><strong>（1）在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">[hep@hadoop102 hadoop]$ touch dfs.hosts</span><br><span class="line">[hep@hadoop102 hadoop]$ vi dfs.hosts</span><br><span class="line"></span><br><span class="line">添加如下主机名称（不添加hadoop105）</span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>

<p><strong>（2）在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>（3）配置文件分发</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop]$ xsync hdfs-site.xml</span><br></pre></td></tr></table></figure>

<p><strong>（4）刷新NameNode</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</span><br><span class="line"></span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure>

<p><strong>（5）更新ResourceManager节点</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<p><strong>（6）在web浏览器上查看</strong></p>
<h4 id="2-黑名单退役"><a href="#2-黑名单退役" class="headerlink" title="2. 黑名单退役"></a>2. 黑名单退役</h4><p>在黑名单上面的主机都会被强制退出。</p>
<p><strong>(1)在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop]$ pwd</span><br><span class="line">/opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop]$ touch dfs.hosts.exclude</span><br><span class="line">[hep@hadoop102 hadoop]$ vi dfs.hosts.exclude</span><br></pre></td></tr></table></figure>

<p>添加如下主机名称（要退役的节点）</p>
<p>hadoop105</p>
<p><strong>(2)在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>(3)刷新NameNode、刷新ResourceManager</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes</span><br><span class="line"></span><br><span class="line">Refresh nodes successful</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<p><strong>(4)检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点。</strong></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/22.png" alt></p>
<p><strong>(5)等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役。</strong></p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/23.png" alt></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop105 hadoop-2.7.2]$ sbin/hadoop-daemon.sh stop datanode</span><br><span class="line">stopping datanode</span><br><span class="line"></span><br><span class="line">[hep@hadoop105 hadoop-2.7.2]$ sbin/yarn-daemon.sh stop nodemanager</span><br><span class="line">stopping nodemanager</span><br></pre></td></tr></table></figure>

<p><strong>(6)如果数据不均衡，可以用命令实现集群的再平衡</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/start-balancer.sh</span><br></pre></td></tr></table></figure>

<p>​    注意：不允许白名单和黑名单中同时出现同一个主机名称。</p>
<h3 id="6-6-Datanode多目录配置"><a href="#6-6-Datanode多目录配置" class="headerlink" title="6.6 Datanode多目录配置"></a>6.6 Datanode多目录配置</h3><p><strong>1.DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</strong></p>
<p><strong>2.具体配置在 hdfs-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>				 	     	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="第7章-HDFS-2-X新特性"><a href="#第7章-HDFS-2-X新特性" class="headerlink" title="第7章 HDFS 2.X新特性"></a>第7章 HDFS 2.X新特性</h2><h3 id="7-1-集群间数据拷贝"><a href="#7-1-集群间数据拷贝" class="headerlink" title="7.1 集群间数据拷贝"></a>7.1 集群间数据拷贝</h3><h4 id="1．scp实现两个远程主机之间的文件复制"><a href="#1．scp实现两个远程主机之间的文件复制" class="headerlink" title="1．scp实现两个远程主机之间的文件复制"></a>1．scp实现两个远程主机之间的文件复制</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">	 scp -r hello.txt [root@hadoop103:/user/hep/hello.txt](mailto:root@hadoop103:/user/hep/hello.txt)       // 推 push</span><br><span class="line"></span><br><span class="line">​    scp -r [root@hadoop103:/user/hep/hello.txt hello.txt](mailto:root@hadoop103:/user/hep/hello.txt  hello.txt)      // 拉 pull</span><br><span class="line"></span><br><span class="line">​    scp -r [root@hadoop103:/user/hep/hello.txt](mailto:root@hadoop103:/user/hep/hello.txt) root@hadoop104:/user/hep  //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。</span><br></pre></td></tr></table></figure>

<h4 id="2．采用distcp命令实现两个Hadoop集群之间的递归数据复制"><a href="#2．采用distcp命令实现两个Hadoop集群之间的递归数据复制" class="headerlink" title="2．采用distcp命令实现两个Hadoop集群之间的递归数据复制"></a>2．采用distcp命令实现两个Hadoop集群之间的递归数据复制</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp</span><br><span class="line"></span><br><span class="line">hdfs://haoop102:9000/user/hep/hello.txt hdfs://hadoop103:9000/user/hep/hello.txt</span><br></pre></td></tr></table></figure>

<h3 id="7-2-小文件存档"><a href="#7-2-小文件存档" class="headerlink" title="7.2 小文件存档"></a>7.2 小文件存档</h3><h4 id="1、HDFS存储小文件弊端"><a href="#1、HDFS存储小文件弊端" class="headerlink" title="1、HDFS存储小文件弊端"></a>1、HDFS存储小文件弊端</h4><p>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。 </p>
<h4 id="2、解决存储小文件办法之一"><a href="#2、解决存储小文件办法之一" class="headerlink" title="2、解决存储小文件办法之一"></a>2、解决存储小文件办法之一</h4><p>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存。</p>
<p><img src="/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/24.png" alt></p>
<h4 id="3．案例实操"><a href="#3．案例实操" class="headerlink" title="3．案例实操"></a>3．案例实操</h4><p><strong>（1）需要启动YARN进程</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ start-yarn.sh</span><br></pre></td></tr></table></figure>

<p><strong>（2）归档文件</strong></p>
<p>​    把/user/hep/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/hep/output路径下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName input.har –p /user/hep/input  /user/hep/output</span><br></pre></td></tr></table></figure>

<p><strong>（3）查看归档</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/hep/output/input.har</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/hep/output/input.har</span><br></pre></td></tr></table></figure>

<p><strong>（4）解归档文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/hep/output/input.har/*  /user/hep</span><br></pre></td></tr></table></figure>

<p>[TOC]</p>

      
      <!-- 打赏 -->
      
        <div id="reward-btn">
          打赏
        </div>
        
    </div>
    <footer class="article-footer">
      <a data-url="https://hepn.github.io/2020/01/08/Hadoop%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/" data-id="ck5brh0ac0000bgwi8fkz2ee6"
        class="article-share-link">分享一下</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">个人学习笔记</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/01/09/Hadoop%E4%B9%8BMapReduce%E8%AF%A6%E8%A7%A3/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Hadoop之MapReduce详解
          
        </div>
      </a>
    
    
      <a href="/2020/01/07/%E7%94%A8%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%9A%84%E9%9B%86%E7%BE%A4/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">用虚拟机搭建一个小的集群</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: false,
        app_id: 'S6NJF4gcIAPGXqI8WBrsuNyo-gzGzoHsz',
        app_key: 'FhQqQA8czeNYE55Nh5K5FfGA',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: '给我的文章加点评论吧~',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      
        <canvas id="code_rain_canvas" width="1440" height="300"></canvas>
        <script type="text/javascript" src="/js/code_rain.js">
        <style>
          #code_rain_canvas {
          position: fixed;
          right: 0px;
          bottom: 0px;
          min-width: 100%;
          min-height: 100%;
          height: auto;
          width: auto;
          z-index: 4;
          }
        </style>
        </script>
        
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        Hep
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>总共 <span id="busuanzi_value_site_pv"></span>人次访问.</li>
  <li>您是Hepn Blog的第 <span id="busuanzi_value_site_uv"></span>个小伙伴</span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/hp-side.svg" alt="Hepn Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">Hepn</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">历程</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="http://shenyu-vip.lofter.com" target="_blank" rel="noopener">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: false
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  
  </div>
  <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
  <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
  <script type="text/javascript" src="/js/fireworks.js"></script>
</body>
</html>