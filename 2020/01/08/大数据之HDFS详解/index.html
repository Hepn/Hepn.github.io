<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Hadoop之HDFS详解 |  Hepn Blog
  </title>
  
  <link rel="shortcut icon" href="/hp.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<meta name="generator" content="Hexo 4.2.0"></head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-大数据之HDFS详解" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Hadoop之HDFS详解
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/" class="article-date">
  <time datetime="2020-01-08T05:49:01.000Z" itemprop="datePublished">2020-01-08</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="第1章-HDFS概述"><a href="#第1章-HDFS概述" class="headerlink" title="第1章 HDFS概述"></a>第1章 HDFS概述</h2><h3 id="1-1-HDFS产生背景"><a href="#1-1-HDFS产生背景" class="headerlink" title="1.1 HDFS产生背景"></a>1.1 HDFS产生背景</h3><p>随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。</p>
<h3 id="1-2HDFS的定义"><a href="#1-2HDFS的定义" class="headerlink" title="1.2HDFS的定义"></a>1.2HDFS的定义</h3><p>HDFS（Hadoop Distributed File System），它是一个<strong>文件系统</strong>，用于存储文件，通过目录树来定位文件；<strong>其次，它是分布式的</strong>，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。<br><strong>HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。</strong>适合用来做数据分析，并不适合用来做网盘应用。</p>
<h3 id="1-3HDFS优缺点"><a href="#1-3HDFS优缺点" class="headerlink" title="1.3HDFS优缺点"></a>1.3HDFS优缺点</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/01.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/E:%5Cblog%5Csource_posts%5C%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3%5C02.png" alt></p>
<h3 id="1-4-HDFS组成架构"><a href="#1-4-HDFS组成架构" class="headerlink" title="1.4 HDFS组成架构"></a>1.4 HDFS组成架构</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/03.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/04.png" alt></p>
<h3 id="1-5-HDFS文件块大小"><a href="#1-5-HDFS文件块大小" class="headerlink" title="1.5 HDFS文件块大小"></a>1.5 HDFS文件块大小</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/05.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/06.png" alt></p>
<h2 id="第2章-HDFS的Shell操作"><a href="#第2章-HDFS的Shell操作" class="headerlink" title="第2章 HDFS的Shell操作"></a>第2章 HDFS的Shell操作</h2><h3 id="2-1基本语法"><a href="#2-1基本语法" class="headerlink" title="2.1基本语法"></a>2.1基本语法</h3><p><strong>bin/hadoop fs</strong> 具体命令  或者 <strong>bin/hdfs dfs</strong> 具体命令</p>
<h3 id="2-2常用命令实操"><a href="#2-2常用命令实操" class="headerlink" title="2.2常用命令实操"></a>2.2常用命令实操</h3><h4 id="（0）启动Hadoop集群"><a href="#（0）启动Hadoop集群" class="headerlink" title="（0）启动Hadoop集群"></a>（0）启动Hadoop集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line"></span><br><span class="line">[hep@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="（1）-help：输出这个命令参数"><a href="#（1）-help：输出这个命令参数" class="headerlink" title="（1）-help：输出这个命令参数"></a>（1）-help：输出这个命令参数</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm</span><br></pre></td></tr></table></figure>

<h4 id="（2）-ls-显示目录信息"><a href="#（2）-ls-显示目录信息" class="headerlink" title="（2）-ls: 显示目录信息"></a>（2）-ls: 显示目录信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /</span><br></pre></td></tr></table></figure>

<h4 id="（3）-mkdir：在HDFS上创建目录"><a href="#（3）-mkdir：在HDFS上创建目录" class="headerlink" title="（3）-mkdir：在HDFS上创建目录"></a>（3）-mkdir：在HDFS上创建目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<h4 id="（4）-moveFromLocal：从本地剪切粘贴到HDFS"><a href="#（4）-moveFromLocal：从本地剪切粘贴到HDFS" class="headerlink" title="（4）-moveFromLocal：从本地剪切粘贴到HDFS"></a>（4）-moveFromLocal：从本地剪切粘贴到HDFS</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch kongming.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<h4 id="（5）-appendToFile：追加一个文件到已经存在的文件末尾"><a href="#（5）-appendToFile：追加一个文件到已经存在的文件末尾" class="headerlink" title="（5）-appendToFile：追加一个文件到已经存在的文件末尾"></a>（5）-appendToFile：追加一个文件到已经存在的文件末尾</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ touch liubei.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ vi liubei.txt</span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（6）-cat：显示文件内容"><a href="#（6）-cat：显示文件内容" class="headerlink" title="（6）-cat：显示文件内容"></a>（6）-cat：显示文件内容</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（7）-chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"><a href="#（7）-chgrp-、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限" class="headerlink" title="（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限"></a>（7）-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -chown atguigu:atguigu  /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"><a href="#（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去" class="headerlink" title="（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去"></a>（8）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /</span><br></pre></td></tr></table></figure>

<h4 id="（9）-copyToLocal：从HDFS拷贝到本地"><a href="#（9）-copyToLocal：从HDFS拷贝到本地" class="headerlink" title="（9）-copyToLocal：从HDFS拷贝到本地"></a>（9）-copyToLocal：从HDFS拷贝到本地</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>

<h4 id="（10）-cp-：从HDFS的一个路径拷贝到HDFS的另一个路径"><a href="#（10）-cp-：从HDFS的一个路径拷贝到HDFS的另一个路径" class="headerlink" title="（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径"></a>（10）-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt</span><br></pre></td></tr></table></figure>

<h4 id="（11）-mv：在HDFS目录中移动文件"><a href="#（11）-mv：在HDFS目录中移动文件" class="headerlink" title="（11）-mv：在HDFS目录中移动文件"></a>（11）-mv：在HDFS目录中移动文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/</span><br></pre></td></tr></table></figure>

<h4 id="（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地"><a href="#（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地" class="headerlink" title="（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地"></a>（12）-get：等同于copyToLocal，就是从HDFS下载文件到本地</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br></pre></td></tr></table></figure>

<h4 id="（13）-getmerge：合并下载多个文件"><a href="#（13）-getmerge：合并下载多个文件" class="headerlink" title="（13）-getmerge：合并下载多个文件"></a>（13）-getmerge：合并下载多个文件</h4><p>比如HDFS的目录 /user/hep/test下有多个文件:log.1, log.2,log.3,…</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/hep/test/* ./zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<h4 id="（14）-put：等同于copyFromLocal"><a href="#（14）-put：等同于copyFromLocal" class="headerlink" title="（14）-put：等同于copyFromLocal"></a>（14）-put：等同于copyFromLocal</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/hep/test/</span><br></pre></td></tr></table></figure>

<h4 id="（15）-tail：显示一个文件的末尾"><a href="#（15）-tail：显示一个文件的末尾" class="headerlink" title="（15）-tail：显示一个文件的末尾"></a>（15）-tail：显示一个文件的末尾</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt</span><br></pre></td></tr></table></figure>

<h4 id="（16）-rm：删除文件或文件夹"><a href="#（16）-rm：删除文件或文件夹" class="headerlink" title="（16）-rm：删除文件或文件夹"></a>（16）-rm：删除文件或文件夹</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/hep/test/jinlian2.txt</span><br></pre></td></tr></table></figure>

<h4 id="（17）-rmdir：删除空目录"><a href="#（17）-rmdir：删除空目录" class="headerlink" title="（17）-rmdir：删除空目录"></a>（17）-rmdir：删除空目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test</span><br></pre></td></tr></table></figure>

<h4 id="（18）-du统计文件夹的大小信息"><a href="#（18）-du统计文件夹的大小信息" class="headerlink" title="（18）-du统计文件夹的大小信息"></a>（18）-du统计文件夹的大小信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/hep/test</span><br></pre></td></tr></table></figure>

<p><strong>结果：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2.7 K /user/atguigu/test</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /user/hep/test</span><br></pre></td></tr></table></figure>

<p><strong>结果：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.3 K &#x2F;user&#x2F;hep&#x2F;test&#x2F;README.txt</span><br><span class="line">15   &#x2F;user&#x2F;hep&#x2F;test&#x2F;jinlian.txt</span><br><span class="line">1.4 K &#x2F;user&#x2F;hep&#x2F;test&#x2F;zaiyiqi.txt</span><br></pre></td></tr></table></figure>

<h4 id="（19）-setrep：设置HDFS中文件的副本数量"><a href="#（19）-setrep：设置HDFS中文件的副本数量" class="headerlink" title="（19）-setrep：设置HDFS中文件的副本数量"></a>（19）-setrep：设置HDFS中文件的副本数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 &#x2F;sanguo&#x2F;shuguo&#x2F;kongming.txt</span><br></pre></td></tr></table></figure>

<p>这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p>
<h2 id="第3章-HDFS客户端操作"><a href="#第3章-HDFS客户端操作" class="headerlink" title="第3章 HDFS客户端操作"></a>第3章 HDFS客户端操作</h2><h3 id="3-1-HDFS客户端环境准备"><a href="#3-1-HDFS客户端环境准备" class="headerlink" title="3.1 HDFS客户端环境准备"></a>3.1 HDFS客户端环境准备</h3><h4 id="1．拷贝对应的编译后的hadoop-jar包到非中文路径"><a href="#1．拷贝对应的编译后的hadoop-jar包到非中文路径" class="headerlink" title="1．拷贝对应的编译后的hadoop jar包到非中文路径"></a>1．拷贝对应的编译后的hadoop jar包到非中文路径</h4><h4 id="2．配置HADOOP-HOME环境变量"><a href="#2．配置HADOOP-HOME环境变量" class="headerlink" title="2．配置HADOOP_HOME环境变量"></a>2．配置HADOOP_HOME环境变量</h4><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/07.png" alt></p>
<h4 id="3-配置Path环境变量"><a href="#3-配置Path环境变量" class="headerlink" title="3.配置Path环境变量"></a>3.配置Path环境变量</h4><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/08.png" alt></p>
<h4 id="4-执行winutils判断是否配置成功"><a href="#4-执行winutils判断是否配置成功" class="headerlink" title="4.执行winutils判断是否配置成功"></a>4.执行winutils判断是否配置成功</h4><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/09.png" alt></p>
<h4 id="5-创建一个Maven工程HdfsClient"><a href="#5-创建一个Maven工程HdfsClient" class="headerlink" title="5.创建一个Maven工程HdfsClient"></a>5.创建一个Maven工程HdfsClient</h4><h4 id="6-导入相应的依赖坐标-日志添加"><a href="#6-导入相应的依赖坐标-日志添加" class="headerlink" title="6.导入相应的依赖坐标+日志添加"></a>6.导入相应的依赖坐标+日志添加</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">​     <span class="tag">&lt;<span class="name">systemPath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systemPath</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：如果Eclipse/Idea打印不出日志，在控制台上只显示</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">1.log4j</span>:<span class="string">WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). </span></span><br><span class="line"><span class="meta">2.log4j</span>:<span class="string">WARN Please initialize the log4j system properly. </span></span><br><span class="line"><span class="meta">3.log4j</span>:<span class="string">WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span></span><br></pre></td></tr></table></figure>

<p><strong>需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="meta">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure>

<h4 id="7-创建包名：com-hep-hdfs"><a href="#7-创建包名：com-hep-hdfs" class="headerlink" title="7.创建包名：com.hep.hdfs"></a>7.创建包名：com.hep.hdfs</h4><h4 id="8-创建HdfsClient类"><a href="#8-创建HdfsClient类" class="headerlink" title="8.创建HdfsClient类"></a>8.创建HdfsClient类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span>&#123;  </span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdirs</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line">   <span class="comment">// 1 获取文件系统</span></span><br><span class="line">   Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">   <span class="comment">// 配置在集群上运行</span></span><br><span class="line">   <span class="comment">// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");</span></span><br><span class="line">   <span class="comment">// FileSystem fs = FileSystem.get(configuration);</span></span><br><span class="line">   FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line">   <span class="comment">// 2 创建目录</span></span><br><span class="line">   fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/new"</span>));</span><br><span class="line">   <span class="comment">// 3 关闭资源</span></span><br><span class="line">   fs.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2HDFS的API操作"><a href="#3-2HDFS的API操作" class="headerlink" title="3.2HDFS的API操作"></a>3.2HDFS的API操作</h3><h4 id="1-HDFS文件下载"><a href="#1-HDFS文件下载" class="headerlink" title="1.HDFS文件下载"></a>1.HDFS文件下载</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//获取一个HDFS的抽象封装对象</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fileSystem = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line"><span class="comment">//用这个对象操作文件系统</span></span><br><span class="line"> fileSystem.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/test"</span>), <span class="keyword">new</span> Path(<span class="string">"e:\\"</span>));</span><br><span class="line"><span class="comment">//关闭文件系统</span></span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-HDFS文件名更改"><a href="#2-HDFS文件名更改" class="headerlink" title="2.HDFS文件名更改"></a>2.HDFS文件名更改</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">FileSystem fileSystem = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"hep"</span>);</span><br><span class="line">fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/test"</span>), <span class="keyword">new</span> Path(<span class="string">"/test2"</span>));</span><br><span class="line">fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-HDFS文件删除"><a href="#3-HDFS文件删除" class="headerlink" title="3.HDFS文件删除"></a>3.HDFS文件删除</h4><p>此处因为每一次对文件系统进行操作前都需要获取一个HDFS的抽象封装对象，以及在使用完之后关闭文件系统。于是我在测试前写了个@Before和@After，便于后续的测试。之后的API测试均由生成的fs替代。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> FileSystem fs;</span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">before</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), <span class="keyword">new</span> Configuration(), <span class="string">"hep"</span>);</span><br><span class="line">    System.out.println(<span class="string">"Before!!!!!"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">after</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    s.close();</span><br><span class="line">    System.out.println(<span class="string">"After!!!!!!!"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> delete = fs.delete(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">if</span> (delete) &#123;</span><br><span class="line">        System.out.println(<span class="string">"删除成功"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">"删除失败"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-HDFS文件的追加"><a href="#4-HDFS文件的追加" class="headerlink" title="4.HDFS文件的追加"></a>4.HDFS文件的追加</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    FSDataOutputStream append = fs.append(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>),<span class="number">1024</span>);</span><br><span class="line">    FileInputStream open = <span class="keyword">new</span> FileInputStream(<span class="string">"E://test/1.txt"</span>);</span><br><span class="line">    IOUtils.copyBytes(open,append,<span class="number">1024</span>,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="5-HDFS文件详情查看以及文件和文件夹判断"><a href="#5-HDFS文件详情查看以及文件和文件夹判断" class="headerlink" title="5.HDFS文件详情查看以及文件和文件夹判断"></a>5.HDFS文件详情查看以及文件和文件夹判断</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    FileStatus[] fileStatuses = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line">    <span class="keyword">for</span>(FileStatus fileStatus : fileStatuses)&#123;</span><br><span class="line">        <span class="keyword">if</span>(fileStatus.isFile())&#123;</span><br><span class="line">            System.out.println(<span class="string">"以下信息是一个文件的信息："</span>);</span><br><span class="line">            System.out.println(fileStatus.getPath());</span><br><span class="line">            System.out.println(fileStatus.getLen());</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"这是一个文件夹"</span>);</span><br><span class="line">            System.out.println(fileStatus.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="6-列出路径下所有的文件块信息"><a href="#6-列出路径下所有的文件块信息" class="headerlink" title="6.列出路径下所有的文件块信息"></a>6.列出路径下所有的文件块信息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>),<span class="keyword">true</span>);<span class="comment">//此处的true是指递归深入文件夹中的文件夹(只能查文件，因为文件夹没有block)</span></span><br><span class="line">    <span class="keyword">while</span>(files.hasNext()) &#123;</span><br><span class="line">        LocatedFileStatus file = files.next();</span><br><span class="line">        System.out.println(<span class="string">"============================"</span>);</span><br><span class="line">        System.out.println(file.getPath());</span><br><span class="line">        System.out.println(<span class="string">"块信息："</span>);</span><br><span class="line">        BlockLocation[] blockLocations = file.getBlockLocations();</span><br><span class="line">        <span class="keyword">for</span>(BlockLocation blockLocation : blockLocations)&#123;</span><br><span class="line">            String[] hosts = blockLocation.getHosts();</span><br><span class="line">            System.out.print(<span class="string">"块在"</span>);</span><br><span class="line">            <span class="keyword">for</span> (String host : hosts)&#123;</span><br><span class="line">                System.out.println(host+<span class="string">" "</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="7-HDFS文件上传"><a href="#7-HDFS文件上传" class="headerlink" title="7.HDFS文件上传"></a>7.HDFS文件上传</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">()</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    configuration.setInt(<span class="string">"dfs.replication"</span>,<span class="number">1</span>);<span class="comment">//设置配置文件</span></span><br><span class="line">    fs = FileSystem.get(URI.create(<span class="string">"hdfs://192.168.1.102:9000"</span>), configuration, <span class="string">"hep"</span>);</span><br><span class="line">    fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"e:/test/1.txt"</span>),<span class="keyword">new</span> Path(<span class="string">"/2.txt"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此处的configuration有自定的配置，若api里有配置文件的设置，api的优先级更高，若没有则看配置文件中的配置。</p>
<h2 id="第4章-HDFS的数据流"><a href="#第4章-HDFS的数据流" class="headerlink" title="第4章 HDFS的数据流"></a>第4章 HDFS的数据流</h2><h3 id="4-1-HDFS写数据流程"><a href="#4-1-HDFS写数据流程" class="headerlink" title="4.1 HDFS写数据流程"></a>4.1 HDFS写数据流程</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/10.png" alt></p>
<ol>
<li>客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</li>
<li>NameNode返回是否可以上传。</li>
<li>客户端请求第一个 Block上传到哪几个DataNode服务器上。</li>
<li>NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</li>
<li>客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</li>
<li>dn1、dn2、dn3逐级应答客户端。</li>
<li>客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
</ol>
<h3 id="4-2-HDFS读数据流程"><a href="#4-2-HDFS读数据流程" class="headerlink" title="4.2 HDFS读数据流程"></a>4.2 HDFS读数据流程</h3><p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/11.png" alt></p>
<ol>
<li>客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<h3 id="4-3网络拓扑"><a href="#4-3网络拓扑" class="headerlink" title="4.3网络拓扑"></a>4.3网络拓扑</h3><p>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。</p>
<p><strong>节点距离：两个节点到达最近的共同祖先的距离总和。</strong></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/12.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/13.png" alt></p>
<h3 id="4-4机架感知（副本存储节点选择）"><a href="#4-4机架感知（副本存储节点选择）" class="headerlink" title="4.4机架感知（副本存储节点选择）"></a>4.4机架感知（副本存储节点选择）</h3><blockquote>
<p>For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack.</p>
</blockquote>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/14.png" alt></p>
<h2 id="第5章-NameNode和SecondaryNameNode"><a href="#第5章-NameNode和SecondaryNameNode" class="headerlink" title="第5章 NameNode和SecondaryNameNode"></a>第5章 NameNode和SecondaryNameNode</h2><h3 id="5-1-NN和2NN工作机制"><a href="#5-1-NN和2NN工作机制" class="headerlink" title="5.1 NN和2NN工作机制"></a>5.1 NN和2NN工作机制</h3><blockquote>
<p><strong>思考：NameNode中的元数据是存储在哪里的？</strong></p>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。<strong>因此产生在磁盘中备份元数据的FsImage。</strong></p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失<strong>。因此，引入Edits文件(只进行追加操作，效率很高)。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。</strong>这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。<strong>因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。</strong></p>
</blockquote>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/15.png" alt></p>
<h4 id="1-第一阶段：NameNode启动"><a href="#1-第一阶段：NameNode启动" class="headerlink" title="1.第一阶段：NameNode启动"></a>1.第一阶段：NameNode启动</h4><p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>（2）客户端对元数据进行增删改的请求。<br>（3）NameNode记录操作日志，更新滚动日志。<br>（4）NameNode在内存中对元数据进行增删改。</p>
<h4 id="2-第二阶段：Secondary-NameNode工作"><a href="#2-第二阶段：Secondary-NameNode工作" class="headerlink" title="2.第二阶段：Secondary NameNode工作"></a>2.第二阶段：Secondary NameNode工作</h4><p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。<br>（2）Secondary NameNode请求执行CheckPoint。<br>（3）NameNode滚动正在写的Edits日志。<br>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。<br>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。<br>（6）生成新的镜像文件fsimage.chkpoint。<br>（7）拷贝fsimage.chkpoint到NameNode。<br>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<hr>
<blockquote>
<p><strong>NN和2NN工作机制详解：</strong></p>
<p><strong>Fsimage：</strong>NameNode内存中元数据序列化后形成的文件。</p>
<p><strong>Edits：</strong>记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。</p>
<p>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（<strong>所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage</strong>）。SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</p>
<p>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（<strong>触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了</strong>）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p>
</blockquote>
<h3 id="5-2-Fsimage和Edits解析"><a href="#5-2-Fsimage和Edits解析" class="headerlink" title="5.2 Fsimage和Edits解析"></a>5.2 Fsimage和Edits解析</h3><h4 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h4><p>​    NameNode被格式化之后，将在<strong>/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录</strong>中产生如下文件：<br><strong>（1）Fsimage文件：</strong>HDFS文件系统元数据的一个<strong>永久性的检查点</strong>，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。<br><strong>（2）Edits文件：</strong>存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。<br><strong>（3）seen_txid文件：</strong>保存的是一个数字，就是最后一个edits_的数字<br>（4）每次NameNode<strong>启动的时候</strong>都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。</p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/16.png" alt></p>
<h4 id="2-oiv查看Fsimage文件"><a href="#2-oiv查看Fsimage文件" class="headerlink" title="2.oiv查看Fsimage文件"></a>2.oiv查看Fsimage文件</h4><h5 id="（1）查看oiv和oev命令"><a href="#（1）查看oiv和oev命令" class="headerlink" title="（1）查看oiv和oev命令"></a>（1）查看oiv和oev命令</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 current]$ hdfs</span><br><span class="line"></span><br><span class="line">**oiv**  apply the offline fsimage viewer to an fsimage</span><br><span class="line"></span><br><span class="line">**oev**  apply the offline edits viewer to an edits file</span><br></pre></td></tr></table></figure>

<h5 id="（2）基本语法"><a href="#（2）基本语法" class="headerlink" title="（2）基本语法"></a>（2）基本语法</h5><p><strong>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</strong></p>
<h5 id="（3）案例实操"><a href="#（3）案例实操" class="headerlink" title="（3）案例实操"></a>（3）案例实操</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hep@hadoop102 current]$  hdfs oiv -p XML -i fsimage_0000000000000000991 -o /opt/module/hadoop-2.7.2/fsimage.xml</span><br><span class="line"></span><br><span class="line">[hep@hadoop102 current]$  hdfs oev -p XML -i edits_0000000000000000888-0000000000000000940 -o /opt/module/hadoop-2.7.2/edits.xml</span><br></pre></td></tr></table></figure>

<p>将显示的xml文件内容打开,部分显示结果如下。</p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/17.png" alt></p>
<p><img src="/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/18.png" alt></p>
<h3 id="5-3-CheckPoint时间设置"><a href="#5-3-CheckPoint时间设置" class="headerlink" title="5.3 CheckPoint时间设置"></a>5.3 CheckPoint时间设置</h3><h4 id="1-通常情况下，SecondaryNameNode每隔一小时执行一次。"><a href="#1-通常情况下，SecondaryNameNode每隔一小时执行一次。" class="headerlink" title="1.通常情况下，SecondaryNameNode每隔一小时执行一次。"></a>1.通常情况下，SecondaryNameNode每隔一小时执行一次。</h4><p>​    <strong>[hdfs-default.xml]</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="2-自定义设置"><a href="#2-自定义设置" class="headerlink" title="2.自定义设置"></a>2.自定义设置</h4><p>一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span> &gt;</span></span><br></pre></td></tr></table></figure>
      
      <!-- 打赏 -->
      
        <div id="reward-btn">
          打赏
        </div>
        
    </div>
    <footer class="article-footer">
      <a data-url="https://hepn.github.io/2020/01/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E8%AF%A6%E8%A7%A3/" data-id="ck555cmgy0000f0wi8at00q1r"
        class="article-share-link">分享一下</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HDFS/" rel="tag">HDFS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">个人学习笔记</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
    
      <a href="/2020/01/07/%E7%94%A8%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%9A%84%E9%9B%86%E7%BE%A4/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">用虚拟机搭建一个小的集群</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: false,
        app_id: 'S6NJF4gcIAPGXqI8WBrsuNyo-gzGzoHsz',
        app_key: 'FhQqQA8czeNYE55Nh5K5FfGA',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: '给我的文章加点评论吧~',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        Hep
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>总共 <span id="busuanzi_value_site_pv"></span>人次访问.</li>
  <li>您是Hepn Blog的第 <span id="busuanzi_value_site_uv"></span>个小伙伴</span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/hp-side.svg" alt="Hepn Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">Hepn</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">历程</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="http://shenyu-vip.lofter.com" target="_blank" rel="noopener">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: false
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  
  </div>
       <!--代码雨-->
 
  <!-- 数字雨 -->
  <canvas id="code_rain_canvas" width="1440" height="900"></canvas>
  <script type="text/javascript" src="/js/code_rain.js">
    <style>
        #code_rain_canvas {
          position: fixed;
          right: 0px;
          bottom: 0px;
          min-width: 100%;
          min-height: 100%;
          height: auto;
          width: auto;
          z-index: 4;
        }
    </style>
</script>

  <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
  <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
  <script type="text/javascript" src="/js/fireworks.js"></script>
</body>
</html>